"""
ë‰´ìŠ¤ ë¶„ì„ ëŒ€ì‹œë³´ë“œ (Gradio)
ì£¼ì œë¥¼ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ìš”ì•½, í‚¤ì›Œë“œ ì¶”ì¶œ, ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

# ===== í™˜ê²½ ì„¤ì • ë° ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ =====
from dotenv import load_dotenv
load_dotenv()

import os
import re
import json
import warnings
from typing import List, Dict, Any, Optional, Literal
from textwrap import dedent
from pprint import pprint
import datetime
from email.utils import parsedate_to_datetime
from dateutil import parser as date_parser
import pytz

warnings.filterwarnings("ignore")

# ===== ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ =====
import requests
import gradio as gr
import matplotlib.pyplot as plt
import bs4
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import WebBaseLoader

# ===== LLM ì´ˆê¸°í™” =====
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ===== í•œê¸€ í°íŠ¸ ì„¤ì • =====
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False


# ===== ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ =====
def naver_news_search(query: str) -> List[Dict]:
    """ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    Args:
        query (str): ê²€ìƒ‰ì–´

    Returns:
        List[Dict]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": os.getenv("NAVER_CLIENT_ID"),
        "X-Naver-Client-Secret": os.getenv("NAVER_CLIENT_SECRET")
    }
    params = {
        "query": query,
        "display": 20,
    }

    response = requests.get(url, headers=headers, params=params)

    if response.status_code != 200:
        print(f"Error: {response.status_code}")
        return []
    
    return response.json()["items"]


# ===== ë‚ ì§œ íŒŒì‹± í•¨ìˆ˜ =====
def parse_date_flexible(date_str: str) -> Optional[datetime.datetime]:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
    if not date_str or not isinstance(date_str, str):
        return None
        
    # 1. RFC 822 í˜•ì‹ ì‹œë„
    try:
        return parsedate_to_datetime(date_str)
    except (ValueError, TypeError):
        pass
    
    # 2. ISO í˜•ì‹ ì‹œë„
    try:
        return datetime.datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except (ValueError, TypeError):
        pass
    
    # 3. í•œê¸€ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
    korean_pattern = r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼\s*(ì˜¤ì „|ì˜¤í›„)?\s*(\d{1,2})ì‹œ\s*(\d{1,2})ë¶„'
    match = re.search(korean_pattern, date_str)
    if match:
        year, month, day, ampm, hour, minute = match.groups()
        hour = int(hour)
        if ampm == 'ì˜¤í›„' and hour < 12:
            hour += 12
        elif ampm == 'ì˜¤ì „' and hour == 12:
            hour = 0
        
        try:
            return datetime.datetime(int(year), int(month), int(day), hour, int(minute))
        except (ValueError, TypeError):
            pass
    
    # 4. dateutil íŒŒì„œ ì‚¬ìš©
    try:
        return date_parser.parse(date_str, fuzzy=True)
    except (ValueError, TypeError):
        pass
    
    return None


# ===== ë‚ ì§œ í•„í„°ë§ =====
def filter_news_by_date_naver(news_data: List[Dict[str, Any]], days_limit: int = 7) -> List[Dict[str, Any]]:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API ê²°ê³¼ë¥¼ ìµœê·¼ ê¸°ê°„ìœ¼ë¡œ í•„í„°ë§"""
    if not news_data:
        return []
    
    filtered_news = []
    current_date = datetime.datetime.now()
    cutoff_date = current_date - datetime.timedelta(days=days_limit)
    
    for i, news in enumerate(news_data):
        pub_date = news.get('pubDate')
        
        if not pub_date:
            continue
            
        try:
            news_date = parse_date_flexible(pub_date)
            
            if not news_date:
                continue
            
            if news_date.tzinfo is not None:
                news_date = news_date.replace(tzinfo=None)
                        
            if news_date >= cutoff_date:
                filtered_news.append(news)
                
        except Exception as e:
            continue
    
    return filtered_news


# ===== í…ìŠ¤íŠ¸ ì •ë¦¬ =====
def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°"""
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    return text


# ===== ì •ë³´ ì¶”ì¶œì„ ìœ„í•œ Pydantic ëª¨ë¸ =====
class ExtractedInfo(BaseModel):
    """ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì¶”ì¶œëœ ì •ë³´"""
    title: str = Field(description="ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©")
    author: Optional[str] = Field(description="ì‘ì„±ì ë˜ëŠ” ì–¸ë¡ ì‚¬")
    published_date: Optional[str] = Field(description="ê²Œì‹œ ë‚ ì§œ")
    summary: str = Field(description="ê¸°ì‚¬ ìš”ì•½ (2-3ë¬¸ì¥)")
    main_content: str = Field(description="ê¸°ì‚¬ì˜ ì£¼ìš” ë‚´ìš© ìš”ì•½")


# ===== ì •ë³´ ì¶”ì¶œ ì²´ì¸ =====
extraction_prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
    - ì œëª©
    - ì‘ì„±ì ë˜ëŠ” ì–¸ë¡ ì‚¬
    - ê²Œì‹œ ë‚ ì§œ
    - ê¸°ì‚¬ ìš”ì•½ (2-3ë¬¸ì¥)
    - ì£¼ìš” ë‚´ìš© ìš”ì•½"""),
    ("human", "{text}")
])

extraction_llm = llm.with_structured_output(ExtractedInfo)
extraction_chain = extraction_prompt | extraction_llm


# ===== ê¸°ì‚¬ ì¶”ì¶œ ë° ì²˜ë¦¬ =====
def extract_articles(query: str, num_articles: int = 3) -> List:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ê¸°ì‚¬ë¥¼ ì¶”ì¶œí•˜ì—¬ ì²˜ë¦¬"""
    
    # ë‰´ìŠ¤ ê²€ìƒ‰
    news_items = naver_news_search(query)
    
    if not news_items:
        return []
    
    # ìµœê·¼ 7ì¼ ì´ë‚´ ë‰´ìŠ¤ë¡œ í•„í„°ë§
    filtered_news = filter_news_by_date_naver(news_items, days_limit=7)
    
    if not filtered_news:
        filtered_news = news_items[:num_articles]
    
    processed_docs = []
    
    for news_item in filtered_news:
        title = clean_text(re.sub(r'<[^>]+>', '', news_item.get('title', '')))
        url = news_item.get('link', '')
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ë§Œ ì²˜ë¦¬
        if "naver.com" not in url:
            continue

        try:
            loader = WebBaseLoader(
                url, 
                header_template={"User-Agent": "Mozilla/5.0"},
                bs_kwargs=dict(
                    parse_only=bs4.SoupStrainer(
                        class_=("media_end_head", "newsct_article _article_body", "go_trans _article_content"),
                    )
                ),
            )
            
            # ë‰´ìŠ¤ ë³¸ë¬¸ ë¡œë“œ
            docs = loader.load()
            if not docs:
                continue
                
            doc = docs[0]

            # ë¶ˆí•„ìš”í•œ ê³µë°± ë¬¸ì ì œê±°
            doc.page_content = clean_text(doc.page_content)
            
            # ì •ë³´ ì¶”ì¶œ ìˆ˜í–‰
            try:
                result = extraction_chain.invoke({"text": doc.page_content})
                
                # ì¶”ì¶œëœ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                doc.metadata["title"] = title
                doc.metadata["source"] = url
                doc.metadata.update(result.model_dump())

                # ì²˜ë¦¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                processed_docs.append(doc)

                # ê¸°ì‚¬ ìˆ˜ ì œí•œ
                if len(processed_docs) >= num_articles:
                    break
            except Exception as e:
                print(f"ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
                
        except Exception as e:
            print(f"ê¸°ì‚¬ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
    
    return processed_docs


# ===== LLM ë¶„ì„ ì²´ì¸ =====

# ìš”ì•½ ì²´ì¸
summarization_prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨í•˜ê³  ì¤‘ìš”í•œ ì •ë³´ëŠ” ìœ ì§€í•˜ì„¸ìš”. (ê³µë°± í¬í•¨ 100ê¸€ì ì´ë‚´ë¡œ ìš”ì•½)"),
    ("human", "{text}")
])
summarization_chain = summarization_prompt | llm | StrOutputParser()


# í‚¤ì›Œë“œ ì¶”ì¶œ
class Keywords(BaseModel):
    """í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œ ëª©ë¡"""
    keywords: List[str] = Field(description="í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ì¤‘ìš” í‚¤ì›Œë“œ ëª©ë¡")

keyword_extraction_prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. í‚¤ì›Œë“œëŠ” í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì£¼ì œì™€ ê°œë…ì„ ë‚˜íƒ€ë‚´ì•¼ í•©ë‹ˆë‹¤. (ìµœëŒ€ 10ê°œ)"),
    ("human", "{text}")
])

keyword_extraction_llm = llm.with_structured_output(Keywords)
keyword_extraction_chain = keyword_extraction_prompt | keyword_extraction_llm


# ê°ì„± ë¶„ì„
class SentimentAnalysis(BaseModel):
    """ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì„± ë¶„ì„ ê²°ê³¼"""
    sentiment: Literal["ê¸ì •ì ", "ë¶€ì •ì ", "ì¤‘ë¦½ì "] = Field(description="ê°ì„± ë¶„ì„ ê²°ê³¼")
    score: float = Field(description="ê°ì„± ì ìˆ˜ (0~1, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê¸ì •ì )")
    key_phrases: List[str] = Field(description="ê¸°ì‚¬ì—ì„œ ê°ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì£¼ìš” êµ¬ë¬¸")
    explanation: str = Field(description="ê°ì„± ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ì„¤ëª…")

sentiment_prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì„±ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ ê¸ì •ì , ë¶€ì •ì , ì¤‘ë¦½ì  ê°ì„±ì„ íŒŒì•…í•˜ì„¸ìš”.
    ê°ì„± ì ìˆ˜ëŠ” 0(ë§¤ìš° ë¶€ì •ì )ì—ì„œ 1(ë§¤ìš° ê¸ì •ì ) ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.
    ê¸°ì‚¬ì—ì„œ ê°ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì£¼ìš” êµ¬ë¬¸ì„ ì¶”ì¶œí•˜ê³  ë¶„ì„ ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ì„¸ìš”."""),
    ("human", "{news_article}")
])

structured_llm = llm.with_structured_output(SentimentAnalysis)
sentiment_analysis_chain = sentiment_prompt | structured_llm


# ===== ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ =====
def process_query(query, num_articles):
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    
    # ì¿¼ë¦¬ ì „ì²˜ë¦¬
    query = query.strip()
    
    if not query:
        return "ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", [], None, "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", {}
    
    try:
        # ë‰´ìŠ¤ ê²€ìƒ‰
        articles = extract_articles(query, int(num_articles))
        
        if not articles:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì£¼ì œë¡œ ì‹œë„í•´ì£¼ì„¸ìš”.", [], None, "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", {}
        
        results = []
        
        # ê° ê¸°ì‚¬ ë¶„ì„
        for article_info in articles:
            article = article_info.page_content
            if article:
                try:
                    # ìš”ì•½
                    summary = summarization_chain.invoke({"text": article})
                    # í‚¤ì›Œë“œ ì¶”ì¶œ
                    keywords = keyword_extraction_chain.invoke({"text": article})
                    # ê°ì„± ë¶„ì„
                    sentiment_analysis = sentiment_analysis_chain.invoke({"news_article": article})
                    
                    # ê²°ê³¼ ì €ì¥
                    result = {
                        "title": article_info.metadata.get("title", "ì œëª© ì—†ìŒ"),
                        "url": article_info.metadata.get("source", ""),
                        "summary": summary,
                        "sentiment": sentiment_analysis.sentiment,
                        "sentiment_score": sentiment_analysis.score,
                        "keywords": keywords.keywords
                    }
                    results.append(result)
                except Exception as e:
                    print(f"ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        if not results:
            return "ê¸°ì‚¬ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", [], None, "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", {}

        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        keyword_counts = {}
        for result in results:
            for keyword in result['keywords']:
                keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1

        # í‚¤ì›Œë“œ ì •ë ¬
        all_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)

        # ê°ì„± ë¶„ì„ ê²°ê³¼ ì¢…í•©
        sentiment_counts = {"ê¸ì •ì ": 0, "ì¤‘ë¦½ì ": 0, "ë¶€ì •ì ": 0}
        for result in results:
            sentiment_counts[result['sentiment']] += 1
        
        # ì°¨íŠ¸ ìƒì„±
        def generate_keyword_chart(keywords):
            """í‚¤ì›Œë“œ ë¹ˆë„ ì°¨íŠ¸ ìƒì„±"""
            if not keywords:
                return None
                
            words = [k[0] for k in keywords[:10]]
            counts = [k[1] for k in keywords[:10]]
            
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.bar(words, counts, color='skyblue')
            ax.set_xticklabels(words, rotation=45, ha='right')
            ax.set_title('ì£¼ìš” í‚¤ì›Œë“œ ë¹ˆë„')
            ax.set_xlabel('í‚¤ì›Œë“œ')
            ax.set_ylabel('ë¹ˆë„ìˆ˜')
            plt.tight_layout()
            
            return fig
        
        keyword_chart = generate_keyword_chart(all_keywords)
        
        # ê²°ê³¼ ìš”ì•½ ìƒì„±
        summary_text = f"ì´ {len(results)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.\n\n"    
        summary_text += f"ê°ì„± ë¶„ì„ ê²°ê³¼: ê¸ì •ì  {sentiment_counts['ê¸ì •ì ']}ê°œ, ì¤‘ë¦½ì  {sentiment_counts['ì¤‘ë¦½ì ']}ê°œ, ë¶€ì •ì  {sentiment_counts['ë¶€ì •ì ']}ê°œ\n\n"
        
        # ê¸°ì‚¬ë³„ ë¶„ì„ ê²°ê³¼ HTML ìƒì„±
        articles_html = ""
        for i, result in enumerate(results, 1):
            articles_html += f"<div style='margin-bottom: 20px; padding: 15px; border: 1px solid #ddd; border-radius: 5px;'>"
            articles_html += f"<h3>{i}. {result['title']}</h3>"
            articles_html += f"<p><strong>URL:</strong> <a href='{result['url']}' target='_blank'>{result['url']}</a></p>"
            articles_html += f"<p><strong>ìš”ì•½:</strong> {result['summary']}</p>"
            articles_html += f"<p><strong>ê°ì„±:</strong> {result['sentiment']} (ì ìˆ˜: {result['sentiment_score']:.2f})</p>"
            
            # í‚¤ì›Œë“œ íƒœê·¸
            articles_html += f"<p><strong>ì£¼ìš” í‚¤ì›Œë“œ:</strong> "
            for keyword in result['keywords'][:5]:
                articles_html += f"<span style='display: inline-block; background-color: #4299e1; color: white; padding: 3px 8px; margin: 2px; border-radius: 10px; font-weight: 500;'>{keyword}</span>"
            articles_html += "</p></div>"
        
        return summary_text, all_keywords, keyword_chart, articles_html, results
        
    except Exception as e:
        error_msg = f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        return error_msg, [], None, error_msg, {}


# ===== Gradio ì¸í„°í˜ì´ìŠ¤ =====
with gr.Blocks(title="ë‰´ìŠ¤ ë¶„ì„ ëŒ€ì‹œë³´ë“œ", theme=gr.themes.Soft(), analytics_enabled=False) as demo:
    gr.Markdown("# ğŸ” ë‰´ìŠ¤ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    gr.Markdown("ì£¼ì œë¥¼ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ìš”ì•½, í‚¤ì›Œë“œ ì¶”ì¶œ, ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
    
    with gr.Row():
        with gr.Column(scale=4):
            query_input = gr.Textbox(
                label="ë¶„ì„í•  ì£¼ì œ ì…ë ¥", 
                placeholder="ì˜ˆ: ì¸ê³µì§€ëŠ¥, ê¸°í›„ë³€í™”, ê²½ì œ ë“±"
            )
        with gr.Column(scale=1):
            num_articles = gr.Slider(
                label="ë¶„ì„í•  ê¸°ì‚¬ ìˆ˜", 
                minimum=1, 
                maximum=10, 
                value=3, 
                step=1
            )
    
    analyze_btn = gr.Button("ë‰´ìŠ¤ ë¶„ì„í•˜ê¸°", variant="primary")
    
    # ê²°ê³¼ ì¶œë ¥ ì˜ì—­
    with gr.Tabs():
        with gr.TabItem("ìš”ì•½ ë° í‚¤ì›Œë“œ"):
            summary_output = gr.Textbox(label="ë¶„ì„ ìš”ì•½", lines=5)
            
            with gr.Row():
                with gr.Column():
                    keywords_output = gr.Dataframe(
                        headers=["í‚¤ì›Œë“œ", "ë¹ˆë„ìˆ˜"],
                        label="ì£¼ìš” í‚¤ì›Œë“œ"
                    )
                with gr.Column():
                    chart_output = gr.Plot(label="í‚¤ì›Œë“œ ë¹ˆë„ ì°¨íŠ¸")
        
        with gr.TabItem("ìƒì„¸ ë¶„ì„ ê²°ê³¼"):
            articles_output = gr.HTML(label="ê¸°ì‚¬ë³„ ë¶„ì„ ê²°ê³¼")
        
        with gr.TabItem("JSON ë°ì´í„°"):
            json_output = gr.JSON(label="ì›ì‹œ ë°ì´í„°")
    
    analyze_btn.click(
        process_query,
        inputs=[query_input, num_articles],
        outputs=[summary_output, keywords_output, chart_output, articles_output, json_output]
    )
    
    gr.Markdown("## ì‚¬ìš© ë°©ë²•")
    gr.Markdown("""
    1. ë¶„ì„í•  ì£¼ì œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤ (ì˜ˆ: 'ì¸ê³µì§€ëŠ¥', 'ê¸°í›„ë³€í™”', 'ê²½ì œ ìœ„ê¸°' ë“±).
    2. ë¶„ì„í•  ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    3. 'ë‰´ìŠ¤ ë¶„ì„í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.
    4. ë¶„ì„ ê²°ê³¼ëŠ” ì„¸ ê°œì˜ íƒ­ì— ë‚˜ëˆ„ì–´ í‘œì‹œë©ë‹ˆë‹¤:
        - ìš”ì•½ ë° í‚¤ì›Œë“œ: ì „ì²´ ë¶„ì„ ìš”ì•½ê³¼ ì£¼ìš” í‚¤ì›Œë“œ ë° ì°¨íŠ¸
        - ìƒì„¸ ë¶„ì„ ê²°ê³¼: ê° ê¸°ì‚¬ë³„ ë¶„ì„ ë‚´ìš© (ìš”ì•½, ê°ì„±, í‚¤ì›Œë“œ)
        - JSON ë°ì´í„°: ì›ì‹œ ë¶„ì„ ë°ì´í„°
    """)


if __name__ == "__main__":
    demo.launch(share=False)
