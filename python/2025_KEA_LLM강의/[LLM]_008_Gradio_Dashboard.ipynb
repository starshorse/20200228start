{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a4d481",
   "metadata": {},
   "source": [
    "#   뉴스 분석 대시보드 (Gradio)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2476b",
   "metadata": {},
   "source": [
    "## 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363dfcd6",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfdacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e76942",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2192c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "from textwrap import dedent\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d32cb",
   "metadata": {},
   "source": [
    "`(3) 뉴스 데이터 로드`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 파일에서 데이터 로드\n",
    "import pickle\n",
    "\n",
    "with open(\"processed_news_articles.pkl\", \"rb\") as f:\n",
    "    loaded_docs = pickle.load(f)\n",
    "    print(f\"로드된 문서 수: {len(loaded_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36736278",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf95809",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Gradio 대시보드 구조 만들기**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def process_query(query, num_articles):\n",
    "    \"\"\"사용자 질문을 처리하고 결과를 반환하는 함수\"\"\"\n",
    "\n",
    "    # 쿼리 전처리\n",
    "    query = query.strip()\n",
    "\n",
    "    # 뉴스 기사 검색\n",
    "\n",
    "    # 요약 \n",
    "    summary_output = \"\"\n",
    "\n",
    "    # 키워드 추출\n",
    "    keywords_output = []\n",
    "\n",
    "    # 차트 출력\n",
    "    chart_output = None\n",
    "\n",
    "    # 기사별 분석 결과\n",
    "    articles_output = \"\"\n",
    "\n",
    "    # JSON 데이터\n",
    "    json_output = {}\n",
    "    \n",
    "    return summary_output, keywords_output, chart_output, articles_output, json_output\n",
    "\n",
    "\n",
    "# Gradio 인터페이스 구현\n",
    "with gr.Blocks(title=\"뉴스 분석 대시보드\", theme=gr.themes.Soft(), analytics_enabled=False) as demo:\n",
    "    gr.Markdown(\"# 🔍 뉴스 분석 대시보드\")\n",
    "    gr.Markdown(\"주제를 입력하면 관련 뉴스를 검색하고 요약, 키워드 추출, 감성 분석을 수행합니다.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            query_input = gr.Textbox(label=\"분석할 주제 입력\", placeholder=\"예: 인공지능, 기후변화, 경제 등\")\n",
    "        with gr.Column(scale=1):\n",
    "            num_articles = gr.Slider(label=\"분석할 기사 수\", minimum=1, maximum=10, value=3, step=1)\n",
    "    \n",
    "    analyze_btn = gr.Button(\"뉴스 분석하기\", variant=\"primary\")\n",
    "    \n",
    "    # 결과 출력 영역\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"요약 및 키워드\"):\n",
    "            summary_output = gr.Textbox(label=\"분석 요약\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    keywords_output = gr.Dataframe(\n",
    "                        headers=[\"키워드\", \"빈도수\"],\n",
    "                        label=\"주요 키워드\"\n",
    "                    )\n",
    "                with gr.Column():\n",
    "                    chart_output = gr.Plot(label=\"키워드 빈도 차트\")\n",
    "        \n",
    "        with gr.TabItem(\"상세 분석 결과\"):\n",
    "            articles_output = gr.HTML(label=\"기사별 분석 결과\")\n",
    "        \n",
    "        with gr.TabItem(\"JSON 데이터\"):\n",
    "            json_output = gr.JSON(label=\"원시 데이터\")\n",
    "    \n",
    "    analyze_btn.click(\n",
    "        process_query,\n",
    "        inputs=[query_input, num_articles],\n",
    "        outputs=[summary_output, keywords_output, chart_output, articles_output, json_output]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"## 사용 방법\")\n",
    "    gr.Markdown(\"\"\"\n",
    "    1. 분석할 주제를 입력합니다 (예: '인공지능', '기후변화', '경제 위기' 등).\n",
    "    2. 분석할 뉴스 기사 수를 선택합니다.\n",
    "    3. '뉴스 분석하기' 버튼을 클릭합니다.\n",
    "    4. 분석 결과는 세 개의 탭에 나누어 표시됩니다:\n",
    "        - 요약 및 키워드: 전체 분석 요약과 주요 키워드 및 차트\n",
    "        - 상세 분석 결과: 각 기사별 분석 내용 (요약, 감성, 키워드)\n",
    "        - JSON 데이터: 원시 분석 데이터\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a74a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb293b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **데이터 처리 함수 구현**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf00fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "from langchain_core.tools import tool\n",
    "from typing import Dict, Literal\n",
    "import datetime\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from email.utils import parsedate_to_datetime\n",
    "from dateutil import parser as date_parser\n",
    "import pytz\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e5e3e",
   "metadata": {},
   "source": [
    "`(1) 네이버 뉴스 검색 `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naver_news_search(\n",
    "    query: str,\n",
    "    ) -> Dict[Dict, int]:\n",
    "    \"\"\"네이버 검색 API를 사용하여 뉴스 검색 결과를 조회합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 검색어\n",
    "\n",
    "    Returns:\n",
    "        Dict[Dict, int]: 검색 결과와 상태 코드  \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    url = \"https://openapi.naver.com/v1/search/news.json\"\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": os.getenv(\"NAVER_CLIENT_ID\"),\n",
    "        \"X-Naver-Client-Secret\": os.getenv(\"NAVER_CLIENT_SECRET\")\n",
    "    }\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"display\": 20,\n",
    "        }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    # JSON 응답을 파싱하여 데이터를 반환\n",
    "    return response.json()[\"items\"]  \n",
    "\n",
    "news_items = naver_news_search(\"인공지능\")\n",
    "\n",
    "print(f\"검색된 뉴스 기사 수: {len(news_items)}\")\n",
    "print(f\"첫 번째 뉴스 기사 제목: {news_items[0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46733ee5",
   "metadata": {},
   "source": [
    "`(2) 뉴스 데이터 전처리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_news_by_date_naver(\n",
    "    news_data: List[Dict[str, Any]], \n",
    "    days_limit: int = 7\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    네이버 뉴스 검색 API 결과를 최근 기간으로 필터링하는 함수\n",
    "    \n",
    "    Args:\n",
    "        news_data (List[Dict]): 검색된 뉴스 데이터 리스트\n",
    "        days_limit (int): 최근 몇 일 동안의 뉴스만 필터링할지 (기본값: 7일)\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 최근 기간으로 필터링된 뉴스 데이터 리스트\n",
    "    \"\"\"\n",
    "    if not news_data:\n",
    "        return []\n",
    "    \n",
    "    filtered_news = []\n",
    "\n",
    "    # timezone 정보가 없는(naive) 현재 날짜 사용\n",
    "    current_date = datetime.datetime.now()\n",
    "    cutoff_date = current_date - datetime.timedelta(days=days_limit)\n",
    "    \n",
    "    for i, news in enumerate(news_data):\n",
    "        # 날짜 필드 찾기 (네이버 API는 'pubDate' 필드 사용)\n",
    "        pub_date = news.get('pubDate')\n",
    "        \n",
    "        # 날짜 필드가 없는 경우 포함하지 않음\n",
    "        if not pub_date:\n",
    "            print(f\"뉴스 {i+1}: 날짜 필드 없음\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # 다양한 날짜 형식 처리\n",
    "            news_date = parse_date_flexible(pub_date)\n",
    "            \n",
    "            if not news_date:\n",
    "                print(f\"뉴스 {i+1}: 날짜 형식 인식 불가 - {pub_date}\")\n",
    "                continue\n",
    "            \n",
    "            # timezone 정보가 있는(aware) 날짜를 naive로 변환\n",
    "            if news_date.tzinfo is not None:\n",
    "                news_date = news_date.replace(tzinfo=None)\n",
    "                        \n",
    "            # 설정한 기간 이내의 뉴스만 포함\n",
    "            if news_date >= cutoff_date:\n",
    "                filtered_news.append(news)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"뉴스 {i+1}: 날짜 파싱 오류 - {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return filtered_news\n",
    "\n",
    "def parse_date_flexible(date_str: str) -> Optional[datetime.datetime]:\n",
    "    \"\"\"\n",
    "    다양한 형식의 날짜 문자열을 파싱하는 함수\n",
    "    \n",
    "    Args:\n",
    "        date_str (str): 파싱할 날짜 문자열\n",
    "        \n",
    "    Returns:\n",
    "        Optional[datetime.datetime]: 파싱된 날짜, 파싱 실패 시 None\n",
    "        \n",
    "    Note:\n",
    "        반환된 날짜는 timezone 정보가 포함될 수 있습니다 (offset-aware).\n",
    "        비교하기 전에 timezone 정보를 제거해야 합니다.\n",
    "    \"\"\"\n",
    "    # 빈 문자열 체크\n",
    "    if not date_str or not isinstance(date_str, str):\n",
    "        return None\n",
    "        \n",
    "    # 1. RFC 822 형식 시도 (네이버 API 공식 문서 형식)\n",
    "    try:\n",
    "        return parsedate_to_datetime(date_str)\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # 2. 일반적인 ISO 형식 시도\n",
    "    try:\n",
    "        return datetime.datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # 3. 네이버 뉴스에서 자주 사용하는 형식 처리\n",
    "    # 예: '2023년 10월 15일 오후 2시 30분'\n",
    "    korean_pattern = r'(\\d{4})년\\s*(\\d{1,2})월\\s*(\\d{1,2})일\\s*(오전|오후)?\\s*(\\d{1,2})시\\s*(\\d{1,2})분'\n",
    "    match = re.search(korean_pattern, date_str)\n",
    "    if match:\n",
    "        year, month, day, ampm, hour, minute = match.groups()\n",
    "        hour = int(hour)\n",
    "        if ampm == '오후' and hour < 12:\n",
    "            hour += 12\n",
    "        elif ampm == '오전' and hour == 12:\n",
    "            hour = 0\n",
    "        \n",
    "        try:\n",
    "            return datetime.datetime(int(year), int(month), int(day), hour, int(minute))\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    # 4. 다른 형식의 날짜 (YYYY-MM-DD, YYYY/MM/DD 등)\n",
    "    # dateutil 라이브러리 사용 (pip install python-dateutil 필요)\n",
    "    try:\n",
    "        return date_parser.parse(date_str)\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # 5. 원본 문자열에서 일자만 추출 시도 (마지막 방법)\n",
    "    # 예: \"Mon, 18 Mar 2025 14:30:45 +0900\" 같은 형식에서 날짜만 추출\n",
    "    date_patterns = [\n",
    "        r'(\\d{1,2})\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+(\\d{4})',  # 18 Mar 2025\n",
    "        r'(\\d{4})-(\\d{1,2})-(\\d{1,2})',  # 2025-03-18\n",
    "        r'(\\d{1,2})/(\\d{1,2})/(\\d{4})'   # 3/18/2025 or 18/3/2025\n",
    "    ]\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        match = re.search(pattern, date_str)\n",
    "        if match:\n",
    "            try:\n",
    "                # 재구성된 간단한 형식으로 다시 시도\n",
    "                simplified = ' '.join(match.groups())\n",
    "                return date_parser.parse(simplified)\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    \n",
    "    # 로그에 파싱 실패한 날짜 형식 출력 (디버깅용)\n",
    "    print(f\"파싱 실패한 날짜 형식: {date_str}\")\n",
    "    \n",
    "    # 모든 시도 실패\n",
    "    return None\n",
    "\n",
    "\n",
    "# 최근 5일 내 뉴스만 필터링\n",
    "filtered_news = filter_news_by_date_naver(news_items, days_limit=5)\n",
    "print(f\"필터링된 뉴스 기사 수: {len(filtered_news)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요한 공백 문자 제거\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    텍스트에서 불필요한 공백, 줄바꿈 문자를 제거하는 함수\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_text = str(text)\n",
    "\n",
    "    # 연속된 줄바꿈을 하나로 통합\n",
    "    cleaned_text = re.sub(r'\\n+', '\\n', cleaned_text)\n",
    "\n",
    "    # 연속된 공백을 하나로 통합\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "\n",
    "    # 줄 시작과 끝의 공백 제거\n",
    "    cleaned_text = re.sub(r'^\\s+|\\s+$', '', cleaned_text, flags=re.MULTILINE)\n",
    "\n",
    "    return cleaned_text\n",
    "    \n",
    "\n",
    "# 추출할 정보를 정의하는 Pydantic 모델 생성\n",
    "class NewsArticle(BaseModel):\n",
    "    \"\"\"뉴스 기사에서 추출할 정보.\"\"\"\n",
    "    \n",
    "    media_outlet: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"뉴스 기사를 발행한 언론사 이름\"\n",
    "    )\n",
    "    reporter: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"기사를 작성한 기자의 이름\"\n",
    "    )\n",
    "    content: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"뉴스 기사의 본문 내용\"\n",
    "    )\n",
    "\n",
    "# 추출 프롬프트 템플릿 정의\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 뉴스 기사에서 정보를 추출하는 전문가입니다. \"\n",
    "            \"주어진 텍스트에서 관련 정보만 추출하세요. \"\n",
    "            \"추출할 수 없는 정보가 있다면 해당 필드의 값으로 null을 반환하세요.\"\n",
    "        ),\n",
    "        (\"human\", \"{text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델 및 구조화된 출력 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\") \n",
    "structured_llm = llm.with_structured_output(NewsArticle)\n",
    "\n",
    "# 추출 파이프라인 생성\n",
    "extraction_chain = prompt_template | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f660779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 검색부터 본문 추출까지의 파이프라인 함수\n",
    "def extract_articles(query=str, num_articles=int):\n",
    "    \"\"\"사용자 질문을 처리하고 결과를 반환하는 함수\"\"\"\n",
    "\n",
    "    # 쿼리 전처리\n",
    "    query = query.strip()\n",
    "\n",
    "    # 뉴스 기사 검색\n",
    "    news_items = naver_news_search(query)\n",
    "    \n",
    "    # 최근 기간으로 필터링\n",
    "    filtered_news = filter_news_by_date_naver(news_items, days_limit=5)\n",
    "\n",
    "    # 각 뉴스 기사에서 정보 추출\n",
    "    processed_docs = []\n",
    "\n",
    "    for news in filtered_news:\n",
    "        url = news['link']\n",
    "        title = news['title']\n",
    "\n",
    "        # 네이버 뉴스 기사 링크에서만 추출\n",
    "        if \"naver.com\" not in url:\n",
    "            continue\n",
    "\n",
    "        loader = WebBaseLoader(\n",
    "            url, \n",
    "            header_template={\"User-Agent\": \"Mozilla/5.0\"},\n",
    "            bs_kwargs=dict(\n",
    "                parse_only=bs4.SoupStrainer(\n",
    "                    class_=(\"media_and_head\", \"newsct_article _article_body\"),\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # 뉴스 본문 로드\n",
    "        doc = loader.load()[0]\n",
    "\n",
    "        # 불필요한 공백 문자 제거\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        \n",
    "        # 정보 추출 수행\n",
    "        result = extraction_chain.invoke({\"text\": doc.page_content})\n",
    "        \n",
    "        # 추출된 정보를 메타데이터에 추가\n",
    "        doc.metadata[\"title\"] = title\n",
    "        doc.metadata.update(result.model_dump())\n",
    "\n",
    "        # 처리된 문서 리스트에 추가\n",
    "        processed_docs.append(doc)\n",
    "\n",
    "        # 기사 수 제한\n",
    "        if len(processed_docs) >= num_articles:\n",
    "            break\n",
    "\n",
    "    \n",
    "    return processed_docs\n",
    "\n",
    "\n",
    "# 테스트\n",
    "query = \"인공지능\"\n",
    "num_articles = 3\n",
    "processed_docs = extract_articles(query, num_articles)\n",
    "print(f\"처리된 문서 수: {len(processed_docs)}\")\n",
    "print(f\"첫 번째 문서 메타데이터: {processed_docs[0].metadata}\")\n",
    "print(f\"첫 번째 문서 본문 내용: {processed_docs[0].page_content[:200]}...\")  # 첫 200자만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103f298",
   "metadata": {},
   "source": [
    "`(3) LLM 데이터 분석 체인`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13352a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 프롬프트 템플릿 정의\n",
    "summarization_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"다음 텍스트를 간결하게 요약하세요. 핵심 내용만 포함하고 중요한 정보는 유지하세요. (공백 포함 100글자 이내로 요약)\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "# 요약 체인 생성\n",
    "summarization_chain = summarization_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aaeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키워드 추출을 위한 Pydantic 모델\n",
    "class Keywords(BaseModel):\n",
    "    \"\"\"텍스트에서 추출된 키워드 목록\"\"\"\n",
    "    keywords: List[str] = Field(description=\"텍스트에서 추출된 중요 키워드 목록 \")\n",
    "\n",
    "# 키워드 추출 프롬프트 템플릿 정의\n",
    "keyword_extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"다음 텍스트에서 중요한 키워드를 추출하세요. 키워드는 텍스트의 핵심 주제와 개념을 나타내야 합니다. (최대 10개)\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "# 구조화된 출력을 위한 LLM 설정\n",
    "keyword_extraction_llm = llm.with_structured_output(Keywords)\n",
    "\n",
    "# 키워드 추출 체인 생성\n",
    "keyword_extraction_chain = keyword_extraction_prompt | keyword_extraction_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8216ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성 분석 결과를 위한 Pydantic 모델\n",
    "class SentimentAnalysis(BaseModel):\n",
    "    \"\"\"뉴스 기사의 감성 분석 결과\"\"\"\n",
    "    sentiment: Literal[\"긍정적\", \"부정적\", \"중립적\"] = Field(description=\"감성 분석 결과 (긍정적, 부정적, 중립적)\")\n",
    "    score: float = Field(description=\"감성 점수 (0~1, 1에 가까울수록 긍정적)\")\n",
    "    key_phrases: List[str] = Field(description=\"기사에서 감성을 나타내는 주요 구문\")\n",
    "    explanation: str = Field(description=\"감성 분석 결과에 대한 설명\")\n",
    "\n",
    "# 구조화된 출력을 위한 LLM 설정\n",
    "structured_llm = llm.with_structured_output(SentimentAnalysis)\n",
    "\n",
    "# 감성 분석 프롬프트 템플릿\n",
    "sentiment_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"당신은 뉴스 기사의 감성을 분석하는 전문가입니다. \n",
    "    주어진 뉴스 기사를 분석하여 긍정적, 부정적, 중립적 감성을 파악하세요.\n",
    "    감성 점수는 0(매우 부정적)에서 1(매우 긍정적) 사이의 값으로 제공하세요.\n",
    "    기사에서 감성을 나타내는 주요 구문을 추출하고 분석 결과를 설명하세요.\"\"\"),\n",
    "    (\"human\", \"{news_article}\")\n",
    "])\n",
    "\n",
    "# 감성 분석 체인 생성\n",
    "sentiment_analysis_chain = sentiment_prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38de44a",
   "metadata": {},
   "source": [
    "`(4) process_query 함수 정의`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38957736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query, num_articles):\n",
    "    \"\"\"사용자 질문을 처리하고 결과를 반환하는 함수\"\"\"\n",
    "\n",
    "    # 쿼리 전처리\n",
    "    query = query.strip()\n",
    "    \n",
    "    if not query:\n",
    "        return \"주제를 입력해주세요.\", [], None, \"분석 결과가 없습니다.\", None\n",
    "    \n",
    "    # 뉴스 검색\n",
    "    articles = extract_articles(query, num_articles)\n",
    "    \n",
    "    if not articles:\n",
    "        return \"검색 결과가 없습니다.\", [], None, \"분석 결과가 없습니다.\", None\n",
    "    \n",
    "    results = []\n",
    "    all_text = \"\"\n",
    "    \n",
    "    # 각 기사 분석\n",
    "    for article_info in articles:\n",
    "        article = article_info.page_content\n",
    "        if article:\n",
    "            # 요약\n",
    "            summary = summarization_chain.invoke({\"text\": article})\n",
    "            # 키워드 추출\n",
    "            keywords = keyword_extraction_chain.invoke({\"text\": article})\n",
    "            # 감성 분석\n",
    "            sentiment_analysis = sentiment_analysis_chain.invoke({\"news_article\": article})\n",
    "            # 결과 저장\n",
    "            result = {\n",
    "                \"title\": article_info.metadata[\"title\"],\n",
    "                \"url\": article_info.metadata[\"source\"],\n",
    "                \"summary\": summary,\n",
    "                \"sentiment\": sentiment_analysis.sentiment,\n",
    "                \"sentiment_score\": sentiment_analysis.score,\n",
    "                \"keywords\": keywords.keywords\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            # 전체 텍스트에 추가\n",
    "            all_text += article + \"\\n\\n\"\n",
    "    \n",
    "    if not results:\n",
    "        return \"기사 분석에 실패했습니다.\", [], None, \"분석 결과가 없습니다.\", None\n",
    "\n",
    "    # 키워드 빈도 계산\n",
    "    keyword_counts = {}\n",
    "    for result in results:\n",
    "        for keyword in result['keywords']:\n",
    "            keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1\n",
    "\n",
    "    # 키워드 정렬\n",
    "    all_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 상위 10개 키워드\n",
    "    top_keywords = all_keywords[:10]\n",
    "\n",
    "\n",
    "    # 감성 분석 결과 종합\n",
    "    sentiment_counts = {\"긍정적\": 0, \"중립적\": 0, \"부정적\": 0}\n",
    "    for result in results:\n",
    "        sentiment_counts[result['sentiment']] += 1\n",
    "\n",
    "    # 한글 폰트 \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.rcParams['font.family'] = 'AppleGothic'\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    # 차트 출력\n",
    "    def generate_keyword_chart(keywords):\n",
    "        \"\"\"키워드 빈도 차트 생성\"\"\"\n",
    "        words = [k[0] for k in keywords]\n",
    "        counts = [k[1] for k in keywords]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(words, counts, color='skyblue')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.title('주요 키워드 빈도')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt   \n",
    "    \n",
    "    keyword_chart = generate_keyword_chart(all_keywords)\n",
    "    \n",
    "    # 결과 요약 생성\n",
    "    summary_text = f\"총 {len(results)}개의 뉴스 기사를 분석했습니다.\\n\\n\"    \n",
    "    summary_text += f\"감성 분석 결과: 긍정적 {sentiment_counts['긍정적']}개, 중립적 {sentiment_counts['중립적']}개, 부정적 {sentiment_counts['부정적']}개\\n\\n\"\n",
    "    \n",
    "    # 결과 목록 생성\n",
    "    articles_html = \"\"\n",
    "    for i, result in enumerate(results, 1):\n",
    "        articles_html += f\"<div style='margin-bottom: 20px; padding: 15px; border: 1px solid #ddd; border-radius: 5px;'>\"\n",
    "        articles_html += f\"<h3>{i}. {result['title']}</h3>\"\n",
    "        articles_html += f\"<p><strong>URL:</strong> <a href='{result['url']}' target='_blank'>{result['url']}</a></p>\"\n",
    "        articles_html += f\"<p><strong>요약:</strong> {result['summary']}</p>\"\n",
    "        articles_html += f\"<p><strong>감성:</strong> {result['sentiment']} (점수: {result['sentiment_score']:.2f})</p>\"\n",
    "        \n",
    "        # 키워드 태그 스타일로 표시\n",
    "        articles_html += f\"<p><strong>주요 키워드:</strong> \"\n",
    "        for idx, (keyword, count) in enumerate(top_keywords):\n",
    "            # 여러 색상으로 순환\n",
    "            colors = ['#4299e1', '#48bb78', '#ed8936', '#9f7aea', '#ed64a6']\n",
    "            bg_color = colors[idx % len(colors)]\n",
    "            articles_html += f\"<span style='display: inline-block; background-color: {bg_color}; color: white; padding: 3px 8px; margin: 2px; border-radius: 10px; font-weight: 500;'>{keyword} ({count})</span>\"\n",
    "        articles_html += \"</p></div>\"\n",
    "    \n",
    "    return summary_text, all_keywords, keyword_chart, articles_html, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "summary_text, all_keywords, keyword_chart, articles_html, results = process_query(\"딥시크 한국 진출\", 3)\n",
    "\n",
    "print(summary_text)\n",
    "print(f\"주요 키워드: {all_keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c3e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6257937",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Gradio 실행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f42ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Gradio 인터페이스 구현\n",
    "with gr.Blocks(title=\"뉴스 분석 대시보드\", theme=gr.themes.Soft(), analytics_enabled=False) as demo:\n",
    "    gr.Markdown(\"# 🔍 뉴스 분석 대시보드\")\n",
    "    gr.Markdown(\"주제를 입력하면 관련 뉴스를 검색하고 요약, 키워드 추출, 감성 분석을 수행합니다.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            query_input = gr.Textbox(label=\"분석할 주제 입력\", placeholder=\"예: 인공지능, 기후변화, 경제 등\")\n",
    "        with gr.Column(scale=1):\n",
    "            num_articles = gr.Slider(label=\"분석할 기사 수\", minimum=1, maximum=10, value=3, step=1)\n",
    "    \n",
    "    analyze_btn = gr.Button(\"뉴스 분석하기\", variant=\"primary\")\n",
    "    \n",
    "    # 결과 출력 영역\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"요약 및 키워드\"):\n",
    "            summary_output = gr.Textbox(label=\"분석 요약\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    keywords_output = gr.Dataframe(\n",
    "                        headers=[\"키워드\", \"빈도수\"],\n",
    "                        label=\"주요 키워드\"\n",
    "                    )\n",
    "                with gr.Column():\n",
    "                    chart_output = gr.Plot(label=\"키워드 빈도 차트\")\n",
    "        \n",
    "        with gr.TabItem(\"상세 분석 결과\"):\n",
    "            articles_output = gr.HTML(label=\"기사별 분석 결과\")\n",
    "        \n",
    "        with gr.TabItem(\"JSON 데이터\"):\n",
    "            json_output = gr.JSON(label=\"원시 데이터\")\n",
    "    \n",
    "    analyze_btn.click(\n",
    "        process_query,\n",
    "        inputs=[query_input, num_articles],\n",
    "        outputs=[summary_output, keywords_output, chart_output, articles_output, json_output]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"## 사용 방법\")\n",
    "    gr.Markdown(\"\"\"\n",
    "    1. 분석할 주제를 입력합니다 (예: '인공지능', '기후변화', '경제 위기' 등).\n",
    "    2. 분석할 뉴스 기사 수를 선택합니다.\n",
    "    3. '뉴스 분석하기' 버튼을 클릭합니다.\n",
    "    4. 분석 결과는 세 개의 탭에 나누어 표시됩니다:\n",
    "        - 요약 및 키워드: 전체 분석 요약과 주요 키워드 및 차트\n",
    "        - 상세 분석 결과: 각 기사별 분석 내용 (요약, 감성, 키워드)\n",
    "        - JSON 데이터: 원시 분석 데이터\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284436f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **[심화] Gradio 인터페이스 구현 및 Hugging Face Spaces 배포** \n",
    "\n",
    "#### 1. **사전 준비**\n",
    "- [Hugging Face](https://huggingface.co/spaces) 계정 생성이 필요 (무료)\n",
    "- 로컬에서 작동하는 Gradio 앱을 준비\n",
    "- 새로운 **가상 환경**에서 실행 (crwal4ai 의존성과 gradio 최신 버전 문제)\n",
    "- **app.py** 파일을 생성하여 프로젝트 폴더로 복사 (.env 파일도 함께 복사)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb9fc4",
   "metadata": {},
   "source": [
    "- pyproject.toml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2498fc5b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "requires-python = \">=3.12,<3.13\"\n",
    "dependencies = [\n",
    "    \"gradio>=5.34.2\",\n",
    "    \"langchain>=0.3.25\",\n",
    "    \"langchain-openai>=0.3.21\",\n",
    "    \"langchain-community>=0.3.24\",\n",
    "    \"langgraph>=0.4.8\",\n",
    "    \"python-dotenv>=1.1.0\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f03ce43",
   "metadata": {},
   "source": [
    "- 터미널에서 다음 명령어 실행"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e37ae6e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "uv sync"
   ]
  },
  {
   "cell_type": "raw",
   "id": "337d58a4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "uv run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d32633",
   "metadata": {},
   "source": [
    "- 서버 종료: 터미널에서 Ctrl + C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e03f37",
   "metadata": {},
   "source": [
    "- requirements.txt 파일 생성 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "29f64e46",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "gradio\n",
    "langchain\n",
    "langchain-openai\n",
    "langchain-community\n",
    "langgraph\n",
    "python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ec80d",
   "metadata": {},
   "source": [
    "- .gitignore 파일 생성"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c76d152c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Python-generated files\n",
    "__pycache__/\n",
    "*.py[oc]\n",
    "build/\n",
    "dist/\n",
    "wheels/\n",
    "*.egg-info\n",
    "\n",
    "# Virtual environments\n",
    ".venv\n",
    ".env\n",
    "\n",
    ".python-version\n",
    "pyproject.toml\n",
    "main.py\n",
    "uv.lock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5897d",
   "metadata": {},
   "source": [
    "#### 2. **배포 방법 (터미널 이용)**\n",
    "- Gradio 앱이 있는 디렉토리로 이동 \n",
    "- 터미널에서 다음 명령어 실행\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "778a6ffb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "uv run gradio deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff10182",
   "metadata": {},
   "source": [
    "- CLI가 안내하는 대로 기본 메타데이터 입력\n",
    "   - HF Token : Hugging Face 계정의 Access Token 복사해서 붙여넣고 엔터 \n",
    "   - git credential 설정 (y/n) : n\n",
    "   - Space 이름\n",
    "   - Any Spaces secrets (y/n) : env 환경변수 설정 (OPENAI_API_KEY 붙여 넣고 엔터)\n",
    "   - 또는 HF Spaces Secrets에 직접 추가 (OPENAI_API_KEY 값 입력 후 엔터)\n",
    "- 배포 완료 후 제공되는 URL로 접근 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c52601",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **[실습 프로젝트]**\n",
    "\n",
    "- 네이버 뉴스 URL을 입력하면, 뉴스 본문을 자동 요약하고 뉴스 감성 분석을 수행하는 과정을 코드로 구현합니다. \n",
    "- Gradio 인터페이스에 적용하여 구현합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
