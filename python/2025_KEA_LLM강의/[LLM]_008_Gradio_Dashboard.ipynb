{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a4d481",
   "metadata": {},
   "source": [
    "#   ë‰´ìŠ¤ ë¶„ì„ ëŒ€ì‹œë³´ë“œ (Gradio)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2476b",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì • ë° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363dfcd6",
   "metadata": {},
   "source": [
    "`(1) Env í™˜ê²½ë³€ìˆ˜`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfdacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e76942",
   "metadata": {},
   "source": [
    "`(2) ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2192c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "from textwrap import dedent\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d32cb",
   "metadata": {},
   "source": [
    "`(3) ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ\n",
    "import pickle\n",
    "\n",
    "with open(\"processed_news_articles.pkl\", \"rb\") as f:\n",
    "    loaded_docs = pickle.load(f)\n",
    "    print(f\"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(loaded_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36736278",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf95809",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Gradio ëŒ€ì‹œë³´ë“œ êµ¬ì¡° ë§Œë“¤ê¸°**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def process_query(query, num_articles):\n",
    "    \"\"\"ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "\n",
    "    # ì¿¼ë¦¬ ì „ì²˜ë¦¬\n",
    "    query = query.strip()\n",
    "\n",
    "    # ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰\n",
    "\n",
    "    # ìš”ì•½ \n",
    "    summary_output = \"\"\n",
    "\n",
    "    # í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    keywords_output = []\n",
    "\n",
    "    # ì°¨íŠ¸ ì¶œë ¥\n",
    "    chart_output = None\n",
    "\n",
    "    # ê¸°ì‚¬ë³„ ë¶„ì„ ê²°ê³¼\n",
    "    articles_output = \"\"\n",
    "\n",
    "    # JSON ë°ì´í„°\n",
    "    json_output = {}\n",
    "    \n",
    "    return summary_output, keywords_output, chart_output, articles_output, json_output\n",
    "\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„\n",
    "with gr.Blocks(title=\"ë‰´ìŠ¤ ë¶„ì„ ëŒ€ì‹œë³´ë“œ\", theme=gr.themes.Soft(), analytics_enabled=False) as demo:\n",
    "    gr.Markdown(\"# ğŸ” ë‰´ìŠ¤ ë¶„ì„ ëŒ€ì‹œë³´ë“œ\")\n",
    "    gr.Markdown(\"ì£¼ì œë¥¼ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ìš”ì•½, í‚¤ì›Œë“œ ì¶”ì¶œ, ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            query_input = gr.Textbox(label=\"ë¶„ì„í•  ì£¼ì œ ì…ë ¥\", placeholder=\"ì˜ˆ: ì¸ê³µì§€ëŠ¥, ê¸°í›„ë³€í™”, ê²½ì œ ë“±\")\n",
    "        with gr.Column(scale=1):\n",
    "            num_articles = gr.Slider(label=\"ë¶„ì„í•  ê¸°ì‚¬ ìˆ˜\", minimum=1, maximum=10, value=3, step=1)\n",
    "    \n",
    "    analyze_btn = gr.Button(\"ë‰´ìŠ¤ ë¶„ì„í•˜ê¸°\", variant=\"primary\")\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥ ì˜ì—­\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"ìš”ì•½ ë° í‚¤ì›Œë“œ\"):\n",
    "            summary_output = gr.Textbox(label=\"ë¶„ì„ ìš”ì•½\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    keywords_output = gr.Dataframe(\n",
    "                        headers=[\"í‚¤ì›Œë“œ\", \"ë¹ˆë„ìˆ˜\"],\n",
    "                        label=\"ì£¼ìš” í‚¤ì›Œë“œ\"\n",
    "                    )\n",
    "                with gr.Column():\n",
    "                    chart_output = gr.Plot(label=\"í‚¤ì›Œë“œ ë¹ˆë„ ì°¨íŠ¸\")\n",
    "        \n",
    "        with gr.TabItem(\"ìƒì„¸ ë¶„ì„ ê²°ê³¼\"):\n",
    "            articles_output = gr.HTML(label=\"ê¸°ì‚¬ë³„ ë¶„ì„ ê²°ê³¼\")\n",
    "        \n",
    "        with gr.TabItem(\"JSON ë°ì´í„°\"):\n",
    "            json_output = gr.JSON(label=\"ì›ì‹œ ë°ì´í„°\")\n",
    "    \n",
    "    analyze_btn.click(\n",
    "        process_query,\n",
    "        inputs=[query_input, num_articles],\n",
    "        outputs=[summary_output, keywords_output, chart_output, articles_output, json_output]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"## ì‚¬ìš© ë°©ë²•\")\n",
    "    gr.Markdown(\"\"\"\n",
    "    1. ë¶„ì„í•  ì£¼ì œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤ (ì˜ˆ: 'ì¸ê³µì§€ëŠ¥', 'ê¸°í›„ë³€í™”', 'ê²½ì œ ìœ„ê¸°' ë“±).\n",
    "    2. ë¶„ì„í•  ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "    3. 'ë‰´ìŠ¤ ë¶„ì„í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.\n",
    "    4. ë¶„ì„ ê²°ê³¼ëŠ” ì„¸ ê°œì˜ íƒ­ì— ë‚˜ëˆ„ì–´ í‘œì‹œë©ë‹ˆë‹¤:\n",
    "        - ìš”ì•½ ë° í‚¤ì›Œë“œ: ì „ì²´ ë¶„ì„ ìš”ì•½ê³¼ ì£¼ìš” í‚¤ì›Œë“œ ë° ì°¨íŠ¸\n",
    "        - ìƒì„¸ ë¶„ì„ ê²°ê³¼: ê° ê¸°ì‚¬ë³„ ë¶„ì„ ë‚´ìš© (ìš”ì•½, ê°ì„±, í‚¤ì›Œë“œ)\n",
    "        - JSON ë°ì´í„°: ì›ì‹œ ë¶„ì„ ë°ì´í„°\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a74a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb293b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ êµ¬í˜„**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf00fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "from langchain_core.tools import tool\n",
    "from typing import Dict, Literal\n",
    "import datetime\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from email.utils import parsedate_to_datetime\n",
    "from dateutil import parser as date_parser\n",
    "import pytz\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e5e3e",
   "metadata": {},
   "source": [
    "`(1) ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naver_news_search(\n",
    "    query: str,\n",
    "    ) -> Dict[Dict, int]:\n",
    "    \"\"\"ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰ì–´\n",
    "\n",
    "    Returns:\n",
    "        Dict[Dict, int]: ê²€ìƒ‰ ê²°ê³¼ì™€ ìƒíƒœ ì½”ë“œ  \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    url = \"https://openapi.naver.com/v1/search/news.json\"\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": os.getenv(\"NAVER_CLIENT_ID\"),\n",
    "        \"X-Naver-Client-Secret\": os.getenv(\"NAVER_CLIENT_SECRET\")\n",
    "    }\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"display\": 20,\n",
    "        }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    # JSON ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ë°ì´í„°ë¥¼ ë°˜í™˜\n",
    "    return response.json()[\"items\"]  \n",
    "\n",
    "news_items = naver_news_search(\"ì¸ê³µì§€ëŠ¥\")\n",
    "\n",
    "print(f\"ê²€ìƒ‰ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜: {len(news_items)}\")\n",
    "print(f\"ì²« ë²ˆì§¸ ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©: {news_items[0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46733ee5",
   "metadata": {},
   "source": [
    "`(2) ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_news_by_date_naver(\n",
    "    news_data: List[Dict[str, Any]], \n",
    "    days_limit: int = 7\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API ê²°ê³¼ë¥¼ ìµœê·¼ ê¸°ê°„ìœ¼ë¡œ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        news_data (List[Dict]): ê²€ìƒ‰ëœ ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
    "        days_limit (int): ìµœê·¼ ëª‡ ì¼ ë™ì•ˆì˜ ë‰´ìŠ¤ë§Œ í•„í„°ë§í• ì§€ (ê¸°ë³¸ê°’: 7ì¼)\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: ìµœê·¼ ê¸°ê°„ìœ¼ë¡œ í•„í„°ë§ëœ ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if not news_data:\n",
    "        return []\n",
    "    \n",
    "    filtered_news = []\n",
    "\n",
    "    # timezone ì •ë³´ê°€ ì—†ëŠ”(naive) í˜„ì¬ ë‚ ì§œ ì‚¬ìš©\n",
    "    current_date = datetime.datetime.now()\n",
    "    cutoff_date = current_date - datetime.timedelta(days=days_limit)\n",
    "    \n",
    "    for i, news in enumerate(news_data):\n",
    "        # ë‚ ì§œ í•„ë“œ ì°¾ê¸° (ë„¤ì´ë²„ APIëŠ” 'pubDate' í•„ë“œ ì‚¬ìš©)\n",
    "        pub_date = news.get('pubDate')\n",
    "        \n",
    "        # ë‚ ì§œ í•„ë“œê°€ ì—†ëŠ” ê²½ìš° í¬í•¨í•˜ì§€ ì•ŠìŒ\n",
    "        if not pub_date:\n",
    "            print(f\"ë‰´ìŠ¤ {i+1}: ë‚ ì§œ í•„ë“œ ì—†ìŒ\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬\n",
    "            news_date = parse_date_flexible(pub_date)\n",
    "            \n",
    "            if not news_date:\n",
    "                print(f\"ë‰´ìŠ¤ {i+1}: ë‚ ì§œ í˜•ì‹ ì¸ì‹ ë¶ˆê°€ - {pub_date}\")\n",
    "                continue\n",
    "            \n",
    "            # timezone ì •ë³´ê°€ ìˆëŠ”(aware) ë‚ ì§œë¥¼ naiveë¡œ ë³€í™˜\n",
    "            if news_date.tzinfo is not None:\n",
    "                news_date = news_date.replace(tzinfo=None)\n",
    "                        \n",
    "            # ì„¤ì •í•œ ê¸°ê°„ ì´ë‚´ì˜ ë‰´ìŠ¤ë§Œ í¬í•¨\n",
    "            if news_date >= cutoff_date:\n",
    "                filtered_news.append(news)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ë‰´ìŠ¤ {i+1}: ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜ - {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return filtered_news\n",
    "\n",
    "def parse_date_flexible(date_str: str) -> Optional[datetime.datetime]:\n",
    "    \"\"\"\n",
    "    ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        date_str (str): íŒŒì‹±í•  ë‚ ì§œ ë¬¸ìì—´\n",
    "        \n",
    "    Returns:\n",
    "        Optional[datetime.datetime]: íŒŒì‹±ëœ ë‚ ì§œ, íŒŒì‹± ì‹¤íŒ¨ ì‹œ None\n",
    "        \n",
    "    Note:\n",
    "        ë°˜í™˜ëœ ë‚ ì§œëŠ” timezone ì •ë³´ê°€ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (offset-aware).\n",
    "        ë¹„êµí•˜ê¸° ì „ì— timezone ì •ë³´ë¥¼ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # ë¹ˆ ë¬¸ìì—´ ì²´í¬\n",
    "    if not date_str or not isinstance(date_str, str):\n",
    "        return None\n",
    "        \n",
    "    # 1. RFC 822 í˜•ì‹ ì‹œë„ (ë„¤ì´ë²„ API ê³µì‹ ë¬¸ì„œ í˜•ì‹)\n",
    "    try:\n",
    "        return parsedate_to_datetime(date_str)\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # 2. ì¼ë°˜ì ì¸ ISO í˜•ì‹ ì‹œë„\n",
    "    try:\n",
    "        return datetime.datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # 3. ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” í˜•ì‹ ì²˜ë¦¬\n",
    "    # ì˜ˆ: '2023ë…„ 10ì›” 15ì¼ ì˜¤í›„ 2ì‹œ 30ë¶„'\n",
    "    korean_pattern = r'(\\d{4})ë…„\\s*(\\d{1,2})ì›”\\s*(\\d{1,2})ì¼\\s*(ì˜¤ì „|ì˜¤í›„)?\\s*(\\d{1,2})ì‹œ\\s*(\\d{1,2})ë¶„'\n",
    "    match = re.search(korean_pattern, date_str)\n",
    "    if match:\n",
    "        year, month, day, ampm, hour, minute = match.groups()\n",
    "        hour = int(hour)\n",
    "        if ampm == 'ì˜¤í›„' and hour < 12:\n",
    "            hour += 12\n",
    "        elif ampm == 'ì˜¤ì „' and hour == 12:\n",
    "            hour = 0\n",
    "        \n",
    "        try:\n",
    "            return datetime.datetime(int(year), int(month), int(day), hour, int(minute))\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    # 4. ë‹¤ë¥¸ í˜•ì‹ì˜ ë‚ ì§œ (YYYY-MM-DD, YYYY/MM/DD ë“±)\n",
    "    # dateutil ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (pip install python-dateutil í•„ìš”)\n",
    "    try:\n",
    "        return date_parser.parse(date_str)\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # 5. ì›ë³¸ ë¬¸ìì—´ì—ì„œ ì¼ìë§Œ ì¶”ì¶œ ì‹œë„ (ë§ˆì§€ë§‰ ë°©ë²•)\n",
    "    # ì˜ˆ: \"Mon, 18 Mar 2025 14:30:45 +0900\" ê°™ì€ í˜•ì‹ì—ì„œ ë‚ ì§œë§Œ ì¶”ì¶œ\n",
    "    date_patterns = [\n",
    "        r'(\\d{1,2})\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+(\\d{4})',  # 18 Mar 2025\n",
    "        r'(\\d{4})-(\\d{1,2})-(\\d{1,2})',  # 2025-03-18\n",
    "        r'(\\d{1,2})/(\\d{1,2})/(\\d{4})'   # 3/18/2025 or 18/3/2025\n",
    "    ]\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        match = re.search(pattern, date_str)\n",
    "        if match:\n",
    "            try:\n",
    "                # ì¬êµ¬ì„±ëœ ê°„ë‹¨í•œ í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„\n",
    "                simplified = ' '.join(match.groups())\n",
    "                return date_parser.parse(simplified)\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    \n",
    "    # ë¡œê·¸ì— íŒŒì‹± ì‹¤íŒ¨í•œ ë‚ ì§œ í˜•ì‹ ì¶œë ¥ (ë””ë²„ê¹…ìš©)\n",
    "    print(f\"íŒŒì‹± ì‹¤íŒ¨í•œ ë‚ ì§œ í˜•ì‹: {date_str}\")\n",
    "    \n",
    "    # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨\n",
    "    return None\n",
    "\n",
    "\n",
    "# ìµœê·¼ 5ì¼ ë‚´ ë‰´ìŠ¤ë§Œ í•„í„°ë§\n",
    "filtered_news = filter_news_by_date_naver(news_items, days_limit=5)\n",
    "print(f\"í•„í„°ë§ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜: {len(filtered_news)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶ˆí•„ìš”í•œ ê³µë°± ë¬¸ì ì œê±°\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°±, ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_text = str(text)\n",
    "\n",
    "    # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ í†µí•©\n",
    "    cleaned_text = re.sub(r'\\n+', '\\n', cleaned_text)\n",
    "\n",
    "    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "\n",
    "    # ì¤„ ì‹œì‘ê³¼ ëì˜ ê³µë°± ì œê±°\n",
    "    cleaned_text = re.sub(r'^\\s+|\\s+$', '', cleaned_text, flags=re.MULTILINE)\n",
    "\n",
    "    return cleaned_text\n",
    "    \n",
    "\n",
    "# ì¶”ì¶œí•  ì •ë³´ë¥¼ ì •ì˜í•˜ëŠ” Pydantic ëª¨ë¸ ìƒì„±\n",
    "class NewsArticle(BaseModel):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì¶”ì¶œí•  ì •ë³´.\"\"\"\n",
    "    \n",
    "    media_outlet: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë°œí–‰í•œ ì–¸ë¡ ì‚¬ ì´ë¦„\"\n",
    "    )\n",
    "    reporter: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"ê¸°ì‚¬ë¥¼ ì‘ì„±í•œ ê¸°ìì˜ ì´ë¦„\"\n",
    "    )\n",
    "    content: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë³¸ë¬¸ ë‚´ìš©\"\n",
    "    )\n",
    "\n",
    "# ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \"\n",
    "            \"ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì„¸ìš”. \"\n",
    "            \"ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” ì •ë³´ê°€ ìˆë‹¤ë©´ í•´ë‹¹ í•„ë“œì˜ ê°’ìœ¼ë¡œ nullì„ ë°˜í™˜í•˜ì„¸ìš”.\"\n",
    "        ),\n",
    "        (\"human\", \"{text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ë° êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„¤ì •\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\") \n",
    "structured_llm = llm.with_structured_output(NewsArticle)\n",
    "\n",
    "# ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
    "extraction_chain = prompt_template | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f660779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‰´ìŠ¤ ê²€ìƒ‰ë¶€í„° ë³¸ë¬¸ ì¶”ì¶œê¹Œì§€ì˜ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜\n",
    "def extract_articles(query=str, num_articles=int):\n",
    "    \"\"\"ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "\n",
    "    # ì¿¼ë¦¬ ì „ì²˜ë¦¬\n",
    "    query = query.strip()\n",
    "\n",
    "    # ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰\n",
    "    news_items = naver_news_search(query)\n",
    "    \n",
    "    # ìµœê·¼ ê¸°ê°„ìœ¼ë¡œ í•„í„°ë§\n",
    "    filtered_news = filter_news_by_date_naver(news_items, days_limit=5)\n",
    "\n",
    "    # ê° ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì •ë³´ ì¶”ì¶œ\n",
    "    processed_docs = []\n",
    "\n",
    "    for news in filtered_news:\n",
    "        url = news['link']\n",
    "        title = news['title']\n",
    "\n",
    "        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ì—ì„œë§Œ ì¶”ì¶œ\n",
    "        if \"naver.com\" not in url:\n",
    "            continue\n",
    "\n",
    "        loader = WebBaseLoader(\n",
    "            url, \n",
    "            header_template={\"User-Agent\": \"Mozilla/5.0\"},\n",
    "            bs_kwargs=dict(\n",
    "                parse_only=bs4.SoupStrainer(\n",
    "                    class_=(\"media_and_head\", \"newsct_article _article_body\"),\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # ë‰´ìŠ¤ ë³¸ë¬¸ ë¡œë“œ\n",
    "        doc = loader.load()[0]\n",
    "\n",
    "        # ë¶ˆí•„ìš”í•œ ê³µë°± ë¬¸ì ì œê±°\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        \n",
    "        # ì •ë³´ ì¶”ì¶œ ìˆ˜í–‰\n",
    "        result = extraction_chain.invoke({\"text\": doc.page_content})\n",
    "        \n",
    "        # ì¶”ì¶œëœ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€\n",
    "        doc.metadata[\"title\"] = title\n",
    "        doc.metadata.update(result.model_dump())\n",
    "\n",
    "        # ì²˜ë¦¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "        processed_docs.append(doc)\n",
    "\n",
    "        # ê¸°ì‚¬ ìˆ˜ ì œí•œ\n",
    "        if len(processed_docs) >= num_articles:\n",
    "            break\n",
    "\n",
    "    \n",
    "    return processed_docs\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "query = \"ì¸ê³µì§€ëŠ¥\"\n",
    "num_articles = 3\n",
    "processed_docs = extract_articles(query, num_articles)\n",
    "print(f\"ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜: {len(processed_docs)}\")\n",
    "print(f\"ì²« ë²ˆì§¸ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°: {processed_docs[0].metadata}\")\n",
    "print(f\"ì²« ë²ˆì§¸ ë¬¸ì„œ ë³¸ë¬¸ ë‚´ìš©: {processed_docs[0].page_content[:200]}...\")  # ì²« 200ìë§Œ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103f298",
   "metadata": {},
   "source": [
    "`(3) LLM ë°ì´í„° ë¶„ì„ ì²´ì¸`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13352a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìš”ì•½ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "summarization_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨í•˜ê³  ì¤‘ìš”í•œ ì •ë³´ëŠ” ìœ ì§€í•˜ì„¸ìš”. (ê³µë°± í¬í•¨ 100ê¸€ì ì´ë‚´ë¡œ ìš”ì•½)\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "# ìš”ì•½ ì²´ì¸ ìƒì„±\n",
    "summarization_chain = summarization_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aaeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ Pydantic ëª¨ë¸\n",
    "class Keywords(BaseModel):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œ ëª©ë¡\"\"\"\n",
    "    keywords: List[str] = Field(description=\"í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ì¤‘ìš” í‚¤ì›Œë“œ ëª©ë¡ \")\n",
    "\n",
    "# í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "keyword_extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. í‚¤ì›Œë“œëŠ” í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì£¼ì œì™€ ê°œë…ì„ ë‚˜íƒ€ë‚´ì•¼ í•©ë‹ˆë‹¤. (ìµœëŒ€ 10ê°œ)\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "# êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ LLM ì„¤ì •\n",
    "keyword_extraction_llm = llm.with_structured_output(Keywords)\n",
    "\n",
    "# í‚¤ì›Œë“œ ì¶”ì¶œ ì²´ì¸ ìƒì„±\n",
    "keyword_extraction_chain = keyword_extraction_prompt | keyword_extraction_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8216ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ìœ„í•œ Pydantic ëª¨ë¸\n",
    "class SentimentAnalysis(BaseModel):\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì„± ë¶„ì„ ê²°ê³¼\"\"\"\n",
    "    sentiment: Literal[\"ê¸ì •ì \", \"ë¶€ì •ì \", \"ì¤‘ë¦½ì \"] = Field(description=\"ê°ì„± ë¶„ì„ ê²°ê³¼ (ê¸ì •ì , ë¶€ì •ì , ì¤‘ë¦½ì )\")\n",
    "    score: float = Field(description=\"ê°ì„± ì ìˆ˜ (0~1, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê¸ì •ì )\")\n",
    "    key_phrases: List[str] = Field(description=\"ê¸°ì‚¬ì—ì„œ ê°ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì£¼ìš” êµ¬ë¬¸\")\n",
    "    explanation: str = Field(description=\"ê°ì„± ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ì„¤ëª…\")\n",
    "\n",
    "# êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ LLM ì„¤ì •\n",
    "structured_llm = llm.with_structured_output(SentimentAnalysis)\n",
    "\n",
    "# ê°ì„± ë¶„ì„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "sentiment_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ê°ì„±ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \n",
    "    ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ ê¸ì •ì , ë¶€ì •ì , ì¤‘ë¦½ì  ê°ì„±ì„ íŒŒì•…í•˜ì„¸ìš”.\n",
    "    ê°ì„± ì ìˆ˜ëŠ” 0(ë§¤ìš° ë¶€ì •ì )ì—ì„œ 1(ë§¤ìš° ê¸ì •ì ) ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.\n",
    "    ê¸°ì‚¬ì—ì„œ ê°ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì£¼ìš” êµ¬ë¬¸ì„ ì¶”ì¶œí•˜ê³  ë¶„ì„ ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\"\"\"),\n",
    "    (\"human\", \"{news_article}\")\n",
    "])\n",
    "\n",
    "# ê°ì„± ë¶„ì„ ì²´ì¸ ìƒì„±\n",
    "sentiment_analysis_chain = sentiment_prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38de44a",
   "metadata": {},
   "source": [
    "`(4) process_query í•¨ìˆ˜ ì •ì˜`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38957736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query, num_articles):\n",
    "    \"\"\"ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "\n",
    "    # ì¿¼ë¦¬ ì „ì²˜ë¦¬\n",
    "    query = query.strip()\n",
    "    \n",
    "    if not query:\n",
    "        return \"ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\", [], None, \"ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\", None\n",
    "    \n",
    "    # ë‰´ìŠ¤ ê²€ìƒ‰\n",
    "    articles = extract_articles(query, num_articles)\n",
    "    \n",
    "    if not articles:\n",
    "        return \"ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\", [], None, \"ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\", None\n",
    "    \n",
    "    results = []\n",
    "    all_text = \"\"\n",
    "    \n",
    "    # ê° ê¸°ì‚¬ ë¶„ì„\n",
    "    for article_info in articles:\n",
    "        article = article_info.page_content\n",
    "        if article:\n",
    "            # ìš”ì•½\n",
    "            summary = summarization_chain.invoke({\"text\": article})\n",
    "            # í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "            keywords = keyword_extraction_chain.invoke({\"text\": article})\n",
    "            # ê°ì„± ë¶„ì„\n",
    "            sentiment_analysis = sentiment_analysis_chain.invoke({\"news_article\": article})\n",
    "            # ê²°ê³¼ ì €ì¥\n",
    "            result = {\n",
    "                \"title\": article_info.metadata[\"title\"],\n",
    "                \"url\": article_info.metadata[\"source\"],\n",
    "                \"summary\": summary,\n",
    "                \"sentiment\": sentiment_analysis.sentiment,\n",
    "                \"sentiment_score\": sentiment_analysis.score,\n",
    "                \"keywords\": keywords.keywords\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            # ì „ì²´ í…ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "            all_text += article + \"\\n\\n\"\n",
    "    \n",
    "    if not results:\n",
    "        return \"ê¸°ì‚¬ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\", [], None, \"ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\", None\n",
    "\n",
    "    # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°\n",
    "    keyword_counts = {}\n",
    "    for result in results:\n",
    "        for keyword in result['keywords']:\n",
    "            keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1\n",
    "\n",
    "    # í‚¤ì›Œë“œ ì •ë ¬\n",
    "    all_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ\n",
    "    top_keywords = all_keywords[:10]\n",
    "\n",
    "\n",
    "    # ê°ì„± ë¶„ì„ ê²°ê³¼ ì¢…í•©\n",
    "    sentiment_counts = {\"ê¸ì •ì \": 0, \"ì¤‘ë¦½ì \": 0, \"ë¶€ì •ì \": 0}\n",
    "    for result in results:\n",
    "        sentiment_counts[result['sentiment']] += 1\n",
    "\n",
    "    # í•œê¸€ í°íŠ¸ \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.rcParams['font.family'] = 'AppleGothic'\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    # ì°¨íŠ¸ ì¶œë ¥\n",
    "    def generate_keyword_chart(keywords):\n",
    "        \"\"\"í‚¤ì›Œë“œ ë¹ˆë„ ì°¨íŠ¸ ìƒì„±\"\"\"\n",
    "        words = [k[0] for k in keywords]\n",
    "        counts = [k[1] for k in keywords]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(words, counts, color='skyblue')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.title('ì£¼ìš” í‚¤ì›Œë“œ ë¹ˆë„')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt   \n",
    "    \n",
    "    keyword_chart = generate_keyword_chart(all_keywords)\n",
    "    \n",
    "    # ê²°ê³¼ ìš”ì•½ ìƒì„±\n",
    "    summary_text = f\"ì´ {len(results)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.\\n\\n\"    \n",
    "    summary_text += f\"ê°ì„± ë¶„ì„ ê²°ê³¼: ê¸ì •ì  {sentiment_counts['ê¸ì •ì ']}ê°œ, ì¤‘ë¦½ì  {sentiment_counts['ì¤‘ë¦½ì ']}ê°œ, ë¶€ì •ì  {sentiment_counts['ë¶€ì •ì ']}ê°œ\\n\\n\"\n",
    "    \n",
    "    # ê²°ê³¼ ëª©ë¡ ìƒì„±\n",
    "    articles_html = \"\"\n",
    "    for i, result in enumerate(results, 1):\n",
    "        articles_html += f\"<div style='margin-bottom: 20px; padding: 15px; border: 1px solid #ddd; border-radius: 5px;'>\"\n",
    "        articles_html += f\"<h3>{i}. {result['title']}</h3>\"\n",
    "        articles_html += f\"<p><strong>URL:</strong> <a href='{result['url']}' target='_blank'>{result['url']}</a></p>\"\n",
    "        articles_html += f\"<p><strong>ìš”ì•½:</strong> {result['summary']}</p>\"\n",
    "        articles_html += f\"<p><strong>ê°ì„±:</strong> {result['sentiment']} (ì ìˆ˜: {result['sentiment_score']:.2f})</p>\"\n",
    "        \n",
    "        # í‚¤ì›Œë“œ íƒœê·¸ ìŠ¤íƒ€ì¼ë¡œ í‘œì‹œ\n",
    "        articles_html += f\"<p><strong>ì£¼ìš” í‚¤ì›Œë“œ:</strong> \"\n",
    "        for idx, (keyword, count) in enumerate(top_keywords):\n",
    "            # ì—¬ëŸ¬ ìƒ‰ìƒìœ¼ë¡œ ìˆœí™˜\n",
    "            colors = ['#4299e1', '#48bb78', '#ed8936', '#9f7aea', '#ed64a6']\n",
    "            bg_color = colors[idx % len(colors)]\n",
    "            articles_html += f\"<span style='display: inline-block; background-color: {bg_color}; color: white; padding: 3px 8px; margin: 2px; border-radius: 10px; font-weight: 500;'>{keyword} ({count})</span>\"\n",
    "        articles_html += \"</p></div>\"\n",
    "    \n",
    "    return summary_text, all_keywords, keyword_chart, articles_html, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸\n",
    "summary_text, all_keywords, keyword_chart, articles_html, results = process_query(\"ë”¥ì‹œí¬ í•œêµ­ ì§„ì¶œ\", 3)\n",
    "\n",
    "print(summary_text)\n",
    "print(f\"ì£¼ìš” í‚¤ì›Œë“œ: {all_keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c3e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6257937",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Gradio ì‹¤í–‰**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f42ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„\n",
    "with gr.Blocks(title=\"ë‰´ìŠ¤ ë¶„ì„ ëŒ€ì‹œë³´ë“œ\", theme=gr.themes.Soft(), analytics_enabled=False) as demo:\n",
    "    gr.Markdown(\"# ğŸ” ë‰´ìŠ¤ ë¶„ì„ ëŒ€ì‹œë³´ë“œ\")\n",
    "    gr.Markdown(\"ì£¼ì œë¥¼ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ìš”ì•½, í‚¤ì›Œë“œ ì¶”ì¶œ, ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            query_input = gr.Textbox(label=\"ë¶„ì„í•  ì£¼ì œ ì…ë ¥\", placeholder=\"ì˜ˆ: ì¸ê³µì§€ëŠ¥, ê¸°í›„ë³€í™”, ê²½ì œ ë“±\")\n",
    "        with gr.Column(scale=1):\n",
    "            num_articles = gr.Slider(label=\"ë¶„ì„í•  ê¸°ì‚¬ ìˆ˜\", minimum=1, maximum=10, value=3, step=1)\n",
    "    \n",
    "    analyze_btn = gr.Button(\"ë‰´ìŠ¤ ë¶„ì„í•˜ê¸°\", variant=\"primary\")\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥ ì˜ì—­\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"ìš”ì•½ ë° í‚¤ì›Œë“œ\"):\n",
    "            summary_output = gr.Textbox(label=\"ë¶„ì„ ìš”ì•½\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    keywords_output = gr.Dataframe(\n",
    "                        headers=[\"í‚¤ì›Œë“œ\", \"ë¹ˆë„ìˆ˜\"],\n",
    "                        label=\"ì£¼ìš” í‚¤ì›Œë“œ\"\n",
    "                    )\n",
    "                with gr.Column():\n",
    "                    chart_output = gr.Plot(label=\"í‚¤ì›Œë“œ ë¹ˆë„ ì°¨íŠ¸\")\n",
    "        \n",
    "        with gr.TabItem(\"ìƒì„¸ ë¶„ì„ ê²°ê³¼\"):\n",
    "            articles_output = gr.HTML(label=\"ê¸°ì‚¬ë³„ ë¶„ì„ ê²°ê³¼\")\n",
    "        \n",
    "        with gr.TabItem(\"JSON ë°ì´í„°\"):\n",
    "            json_output = gr.JSON(label=\"ì›ì‹œ ë°ì´í„°\")\n",
    "    \n",
    "    analyze_btn.click(\n",
    "        process_query,\n",
    "        inputs=[query_input, num_articles],\n",
    "        outputs=[summary_output, keywords_output, chart_output, articles_output, json_output]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"## ì‚¬ìš© ë°©ë²•\")\n",
    "    gr.Markdown(\"\"\"\n",
    "    1. ë¶„ì„í•  ì£¼ì œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤ (ì˜ˆ: 'ì¸ê³µì§€ëŠ¥', 'ê¸°í›„ë³€í™”', 'ê²½ì œ ìœ„ê¸°' ë“±).\n",
    "    2. ë¶„ì„í•  ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "    3. 'ë‰´ìŠ¤ ë¶„ì„í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.\n",
    "    4. ë¶„ì„ ê²°ê³¼ëŠ” ì„¸ ê°œì˜ íƒ­ì— ë‚˜ëˆ„ì–´ í‘œì‹œë©ë‹ˆë‹¤:\n",
    "        - ìš”ì•½ ë° í‚¤ì›Œë“œ: ì „ì²´ ë¶„ì„ ìš”ì•½ê³¼ ì£¼ìš” í‚¤ì›Œë“œ ë° ì°¨íŠ¸\n",
    "        - ìƒì„¸ ë¶„ì„ ê²°ê³¼: ê° ê¸°ì‚¬ë³„ ë¶„ì„ ë‚´ìš© (ìš”ì•½, ê°ì„±, í‚¤ì›Œë“œ)\n",
    "        - JSON ë°ì´í„°: ì›ì‹œ ë¶„ì„ ë°ì´í„°\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284436f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **[ì‹¬í™”] Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ ë° Hugging Face Spaces ë°°í¬** \n",
    "\n",
    "#### 1. **ì‚¬ì „ ì¤€ë¹„**\n",
    "- [Hugging Face](https://huggingface.co/spaces) ê³„ì • ìƒì„±ì´ í•„ìš” (ë¬´ë£Œ)\n",
    "- ë¡œì»¬ì—ì„œ ì‘ë™í•˜ëŠ” Gradio ì•±ì„ ì¤€ë¹„\n",
    "- ìƒˆë¡œìš´ **ê°€ìƒ í™˜ê²½**ì—ì„œ ì‹¤í–‰ (crwal4ai ì˜ì¡´ì„±ê³¼ gradio ìµœì‹  ë²„ì „ ë¬¸ì œ)\n",
    "- **app.py** íŒŒì¼ì„ ìƒì„±í•˜ì—¬ í”„ë¡œì íŠ¸ í´ë”ë¡œ ë³µì‚¬ (.env íŒŒì¼ë„ í•¨ê»˜ ë³µì‚¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb9fc4",
   "metadata": {},
   "source": [
    "- pyproject.toml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2498fc5b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "requires-python = \">=3.12,<3.13\"\n",
    "dependencies = [\n",
    "    \"gradio>=5.34.2\",\n",
    "    \"langchain>=0.3.25\",\n",
    "    \"langchain-openai>=0.3.21\",\n",
    "    \"langchain-community>=0.3.24\",\n",
    "    \"langgraph>=0.4.8\",\n",
    "    \"python-dotenv>=1.1.0\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f03ce43",
   "metadata": {},
   "source": [
    "- í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e37ae6e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "uv sync"
   ]
  },
  {
   "cell_type": "raw",
   "id": "337d58a4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "uv run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d32633",
   "metadata": {},
   "source": [
    "- ì„œë²„ ì¢…ë£Œ: í„°ë¯¸ë„ì—ì„œ Ctrl + C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e03f37",
   "metadata": {},
   "source": [
    "- requirements.txt íŒŒì¼ ìƒì„± "
   ]
  },
  {
   "cell_type": "raw",
   "id": "29f64e46",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "gradio\n",
    "langchain\n",
    "langchain-openai\n",
    "langchain-community\n",
    "langgraph\n",
    "python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ec80d",
   "metadata": {},
   "source": [
    "- .gitignore íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c76d152c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Python-generated files\n",
    "__pycache__/\n",
    "*.py[oc]\n",
    "build/\n",
    "dist/\n",
    "wheels/\n",
    "*.egg-info\n",
    "\n",
    "# Virtual environments\n",
    ".venv\n",
    ".env\n",
    "\n",
    ".python-version\n",
    "pyproject.toml\n",
    "main.py\n",
    "uv.lock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5897d",
   "metadata": {},
   "source": [
    "#### 2. **ë°°í¬ ë°©ë²• (í„°ë¯¸ë„ ì´ìš©)**\n",
    "- Gradio ì•±ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë¡œ ì´ë™ \n",
    "- í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "778a6ffb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "uv run gradio deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff10182",
   "metadata": {},
   "source": [
    "- CLIê°€ ì•ˆë‚´í•˜ëŠ” ëŒ€ë¡œ ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì…ë ¥\n",
    "   - HF Token : Hugging Face ê³„ì •ì˜ Access Token ë³µì‚¬í•´ì„œ ë¶™ì—¬ë„£ê³  ì—”í„° \n",
    "   - git credential ì„¤ì • (y/n) : n\n",
    "   - Space ì´ë¦„\n",
    "   - Any Spaces secrets (y/n) : env í™˜ê²½ë³€ìˆ˜ ì„¤ì • (OPENAI_API_KEY ë¶™ì—¬ ë„£ê³  ì—”í„°)\n",
    "   - ë˜ëŠ” HF Spaces Secretsì— ì§ì ‘ ì¶”ê°€ (OPENAI_API_KEY ê°’ ì…ë ¥ í›„ ì—”í„°)\n",
    "- ë°°í¬ ì™„ë£Œ í›„ ì œê³µë˜ëŠ” URLë¡œ ì ‘ê·¼ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c52601",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **[ì‹¤ìŠµ í”„ë¡œì íŠ¸]**\n",
    "\n",
    "- ë„¤ì´ë²„ ë‰´ìŠ¤ URLì„ ì…ë ¥í•˜ë©´, ë‰´ìŠ¤ ë³¸ë¬¸ì„ ìë™ ìš”ì•½í•˜ê³  ë‰´ìŠ¤ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ê³¼ì •ì„ ì½”ë“œë¡œ êµ¬í˜„í•©ë‹ˆë‹¤. \n",
    "- Gradio ì¸í„°í˜ì´ìŠ¤ì— ì ìš©í•˜ì—¬ êµ¬í˜„í•©ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
