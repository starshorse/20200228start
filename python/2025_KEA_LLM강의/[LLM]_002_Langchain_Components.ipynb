{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LangChain의 개념과 주요 컴포넌트 이해\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LangChain 소개\n",
    "\n",
    "- **LangChain**은 대화형 AI 애플리케이션을 쉽게 개발할 수 있도록 도와주는 프레임워크입니다.\n",
    "\n",
    "- **핵심 가치**\n",
    "    - **모듈화**: 독립적인 컴포넌트를 조합하여 복잡한 AI 시스템 구축\n",
    "    - **상호운용성**: 다양한 AI 모델과 데이터 소스를 하나의 인터페이스로 통합\n",
    "    - **확장성**: 간단한 챗봇부터 복잡한 AI 에이전트까지 확장 가능\n",
    "    - **관찰성**: LangSmith를 통한 실시간 모니터링 및 디버깅\n",
    "\n",
    "- **핵심 아키텍처**\n",
    "    ```markdown\n",
    "    ├── langchain-core     # 기본 추상화 및 인터페이스\n",
    "    ├── langchain          # 체인, 에이전트, 검색 전략\n",
    "    ├── langchain-openai   # OpenAI 통합\n",
    "    ├── LangGraph          # 복잡한 에이전트 워크플로우\n",
    "    └── LangSmith          # 모니터링 및 디버깅\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" \n",
    "        alt=\"langchain_stack\" \n",
    "        width=\"600\" \n",
    "        style=\"border: 0;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LangSmith 모니터링**\n",
    "\n",
    "    - **LangSmith**는 LLM 애플리케이션의 관찰성(Observability)을 제공하는 도구입니다.\n",
    "\n",
    "    - **주요 기능**\n",
    "        - **체인 실행 로깅 및 추적**\n",
    "        - **프롬프트 디버깅**\n",
    "        - **성능 측정 및 분석**\n",
    "        - **실시간 모니터링**\n",
    "\n",
    "    - **계정 가입 및 설정**\n",
    "        ```python\n",
    "        # 1. LangSmith 계정 가입: https://www.langchain.com/langsmith\n",
    "        # 2. .env 파일 설정\n",
    "        LANGSMITH_API_KEY=your_langsmith_api_key\n",
    "        LANGSMITH_TRACING=true\n",
    "        LANGSMITH_PROJECT=your_project_name\n",
    "\n",
    "        # 3. 환경 확인\n",
    "        from dotenv import load_dotenv\n",
    "        import os\n",
    "\n",
    "        load_dotenv()\n",
    "        print(f\"LangSmith 추적 상태: {os.getenv('LANGSMITH_TRACING')}\")\n",
    "        print(f\"프로젝트명: {os.getenv('LANGSMITH_PROJECT')}\")\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 환경 설정\n",
    "\n",
    "- **설치**\n",
    "\n",
    "    ```bash\n",
    "    # pip 설치\n",
    "    pip install langchain langchain-openai langchain-google-genai\n",
    "\n",
    "    # uv 설치 \n",
    "    uv add langchain langchain-openai langchain-google-genai\n",
    "\n",
    "    # 추가 도구 pip 설치 (선택사항)\n",
    "    pip install langchain-ollama langsmith\n",
    "\n",
    "    # 추가 도구 uv 설치 (선택사항)\n",
    "    uv add langchain-ollama langsmith\n",
    "    ``` \n",
    "\n",
    "- **API 키 설정**\n",
    "\n",
    "    ```python\n",
    "    # .env 파일 생성\n",
    "    OPENAI_API_KEY=your_openai_api_key_here\n",
    "    GOOGLE_API_KEY=your_google_api_key_here\n",
    "\n",
    "    # LangSmith 설정 (선택사항)\n",
    "    LANGSMITH_API_KEY=your_langsmith_api_key\n",
    "    LANGSMITH_TRACING=true\n",
    "    LANGSMITH_PROJECT=your_project_name\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 변수 로드\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적 확인\n",
    "import os\n",
    "print(f\"LangSmith 추적: {os.getenv('LANGSMITH_TRACING')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 핵심 컴포넌트\n",
    "\n",
    "### 3.1 Chat Models (채팅 모델)\n",
    "\n",
    "- OpenAI, Anthropic, Google 등 다양한 모델을 지원\n",
    "- 텍스트 생성, 대화, 요약 등의 작업을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\", \n",
    "    temperature=0.3,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "# 간단한 대화\n",
    "response = model.invoke(\"탄소의 원자 번호는 무엇인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"답변: {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"메타데이터: {response.response_metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Messages (메시지)\n",
    "\n",
    "- 메시지는 AI와의 대화에서 역할을 구분하는 기본 단위입니다.\n",
    "- 메시지는 사용자, AI, 시스템 등 다양한 역할을 가질 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# 시스템 메시지: AI의 역할 정의\n",
    "system_msg = SystemMessage(content=\"당신은 친절한 화학 선생님입니다.\")\n",
    "\n",
    "# 사용자 메시지\n",
    "human_msg = HumanMessage(content=\"탄소의 원자 번호는 몇 번인가요?\")\n",
    "\n",
    "# 대화 실행\n",
    "messages = [system_msg, human_msg]\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Prompt Templates (프롬프트 템플릿)\n",
    "\n",
    "- 템플릿을 사용하여 일관된 프롬프트를 생성할 수 있습니다.\n",
    "- 변수 치환을 통해 동적인 프롬프트를 적용하는 데 유용합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 기본 템플릿`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 전문가 템플릿\n",
    "template = \"\"\"\n",
    "당신은 {topic} 분야의 전문가입니다. {topic}에 관한 다음 질문에 답변해주세요.\n",
    "질문: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 템플릿 입력 변수 확인\n",
    "print(f\"필수 변수: {prompt.input_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 템플릿 확인\n",
    "print(f\"템플릿: {prompt.template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 템플릿 사용\n",
    "formatted_prompt = prompt.format(\n",
    "    topic=\"화학\",\n",
    "    question=\"탄소의 원자 번호는 무엇인가요?\"\n",
    ")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 모델에 전달하여 답변 생성\n",
    "response = model.invoke(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 채팅 템플릿`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 채팅용 템플릿\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 전문 {subject} 상담사입니다.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 템플릿 사용\n",
    "prompt = chat_template.format_messages(\n",
    "    subject=\"진로\",\n",
    "    question=\"데이터 사이언티스트가 되려면 어떤 공부를 해야 하나요?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 템플릿 확인\n",
    "pprint(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 모델에 전달하여 답변 생성\n",
    "response = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. LCEL (LangChain Expression Language)\n",
    "\n",
    "### 4.1 LCEL이란?\n",
    "\n",
    "- **LCEL**은 `|` 연산자를 사용하여 컴포넌트들을 순차적으로 연결하는 선언적 체이닝을 지원\n",
    "\n",
    "- **핵심 특징**\n",
    "    - **재사용성**: 정의된 체인을 다른 체인의 컴포넌트로 활용\n",
    "    - **다양한 실행 방식**: `.invoke()`, `.batch()`, `.stream()`, `.astream()`\n",
    "    - **자동 최적화**: 배치 처리 시 효율적인 작업 수행\n",
    "    - **스키마 지원**: 입력/출력 스키마 자동 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 기본 체인 구성\n",
    "\n",
    "`(1) Prompt + LLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 컴포넌트 정의\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"당신은 {topic} 분야의 전문가입니다. {topic}에 관한 다음 질문에 답변해주세요.\\n\"\n",
    "    \"질문: {question}\"\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.3)\n",
    "\n",
    "# 체인 구성\n",
    "chain = prompt | llm\n",
    "\n",
    "# 체인 실행\n",
    "response = chain.invoke({\n",
    "    \"topic\": \"화학\",\n",
    "    \"question\": \"탄소의 원자 번호는 무엇인가요?\"\n",
    "})\n",
    "\n",
    "print(f\"답변: {response.content}\")  # AIMessage 객체로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) Prompt + LLM + Output Parser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 출력 파서 추가\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 완전한 체인 구성\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# 체인 실행 (문자열 반환)\n",
    "response = chain.invoke({\n",
    "    \"topic\": \"화학\",\n",
    "    \"question\": \"탄소의 원자 번호는 무엇인가요?\"\n",
    "})\n",
    "\n",
    "print(f\"답변: {response}\")  # 문자열로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Runnable\n",
    "\n",
    "* 실행 인터페이스: 모든 LangChain 컴포넌트는 Runnable 인터페이스를 구현하여 일관된 방식으로 실행\n",
    "\n",
    "* 실행 메서드: `.invoke()`, `.batch()`, `.stream()`, `.astream()` 등 다양한 실행 방식을 제공\n",
    "\n",
    "* 호환성: 모든 Runnable 컴포넌트는 파이프(|) 연산자를 통해 연결 가능하며, 재사용이 용이\n",
    "\n",
    "* Runnable의 주요 유형:\n",
    "\n",
    "    * `RunnableSequence`: 여러 Runnable을 순차적으로 실행\n",
    "    * `RunnablePassthrough`: 입력을 그대로 다음 단계로 전달    \n",
    "    * `RunnableParallel`: 여러 Runnable을 병렬로 실행\n",
    "    * `RunnableLambda`: 파이썬 함수를 Runnable로 래핑하여 체인에서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 RunnableSequence (순차 실행)\n",
    "\n",
    "- **RunnableSequence**는 컴포넌트들을 연결하여 순차적으로 데이터를 처리하는 체인\n",
    "\n",
    "- `|` 연산자로 연결된 각 단계의 **출력이 다음 단계의 입력**으로 전달\n",
    "\n",
    "- **다양한 실행 방식**(동기/비동기, 배치/스트리밍)을 지원\n",
    "\n",
    "- LLM 체인, 데이터 파이프라인, 자동화된 작업 등 **다단계 처리**에 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 전용 체인 (파이프 연산자 사용)\n",
    "translation_prompt = PromptTemplate.from_template(\n",
    "    \"'{text}'를 영어로 번역해주세요. 번역된 문장만을 출력해주세요.\"\n",
    ")\n",
    "\n",
    "translation_chain = translation_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 번역 실행\n",
    "result = translation_chain.invoke({\"text\": \"좋은 하루 되세요!\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# 명시적 RunnableSequence 생성\n",
    "translation_chain = RunnableSequence(\n",
    "    first=translation_prompt, \n",
    "    middle=[llm],\n",
    "    last=StrOutputParser()\n",
    ")\n",
    "\n",
    "# 파이프 연산자와 동일한 결과\n",
    "# translation_chain = translation_prompt | llm | StrOutputParser()\n",
    "\n",
    "result = translation_chain.invoke({\"text\": \"좋은 하루 되세요!\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 RunnableLambda (함수 래핑)\n",
    "\n",
    "- **RunnableLambda**는 일반 함수를 Runnable 객체로 변환하는 래퍼 컴포넌트\n",
    "\n",
    "- 체인에 **커스텀 로직**을 쉽게 통합할 수 있어 데이터 전처리, 후처리에 유용\n",
    "\n",
    "- `|` 연산자로 다른 컴포넌트들과 연결해 **복잡한 처리 흐름**을 구성 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 텍스트에서 숫자를 추출하는 함수\n",
    "def extract_number(query):\n",
    "    return int(re.search(r'\\d+', query).group())\n",
    "\n",
    "# RunnablePassthrough로 입력을 그대로 전달하고, RunnableLambda로 숫자 추출 함수 실행\n",
    "runnable = RunnableLambda(extract_number)\n",
    "\n",
    "# 입력 텍스트에서 6을 추출\n",
    "result = runnable.invoke('탄소의 원자 번호는 6입니다.')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# 데이터 전처리 함수 정의\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\" 입력 텍스트를 소문자로 변환하고 양쪽 공백을 제거합니다. \"\"\"\n",
    "    return text.strip().lower()\n",
    "\n",
    "# 후처리 함수 정의\n",
    "def postprocess_response(response: AIMessage) -> dict:\n",
    "    \"\"\" 응답 텍스트를 대문자로 변환하고 길이를 계산합니다. \"\"\"\n",
    "    response_text = response.content\n",
    "    return {\n",
    "        \"processed_response\": response_text.upper(),\n",
    "        \"length\": len(response_text)\n",
    "    }\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = ChatPromptTemplate.from_template(\"다음 주제에 대해 영어 한 문장으로 설명해주세요: {topic}\")\n",
    "\n",
    "# 처리 파이프라인 구성\n",
    "chain = (\n",
    "    RunnableLambda(preprocess_text) |  # 입력 전처리\n",
    "    prompt |                           # 프롬프트 포맷팅\n",
    "    llm |                              # LLM 추론\n",
    "    RunnableLambda(postprocess_response)  # 출력 후처리\n",
    ")\n",
    "\n",
    "# 체인 실행\n",
    "result = chain.invoke(\"  Artificial Intelligence  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"응답 길이: {result['length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"처리된 응답: {result['processed_response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 RunnableParallel (병렬 실행)\n",
    "\n",
    "- **RunnableParallel**은 여러 컴포넌트를 딕셔너리 형태로 구성해 **동시 실행**\n",
    "\n",
    "- 동일한 입력이 모든 병렬 컴포넌트에 전달되며, 결과는 **키-값 쌍**으로 반환\n",
    "\n",
    "- **데이터 변환**과 **파이프라인 구성**에 특화되어 있으며, 출력 형식을 다음 단계에 맞게 조정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 질문 분석 체인`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "# Pydantic 모델로 구조화된 출력 정의\n",
    "class SubjectClassification(BaseModel):\n",
    "    \"\"\"질문의 주제 분류 결과\"\"\"\n",
    "    category: Literal[\"화학(Chemistry)\", \"물리(Physics)\", \"생물(Biology)\"] = Field(\n",
    "        description=\"질문이 속하는 과학 분야 카테고리\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"분류 이유에 대한 짧은 설명\"\n",
    "    )\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "classification_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"당신은 질문을 과학 분야로 분류하는 전문가다.\n",
    "        질문을 분석하고 해당하는 카테고리로 정확하게 분류해야 한다.\n",
    "        \n",
    "        분류 기준:\n",
    "        - 화학(Chemistry): 원소, 화합물, 반응, 분자 구조 등\n",
    "        - 물리(Physics): 힘, 운동, 에너지, 파동, 전기 등  \n",
    "        - 생물(Biology): 생명체, 세포, 유전, 생태계 등\"\"\"\n",
    "    ),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 구조화된 출력을 사용한 체인 구성\n",
    "structured_llm = llm.with_structured_output(SubjectClassification)\n",
    "classification_chain = classification_prompt | structured_llm\n",
    "\n",
    "# 사용 예시\n",
    "result = classification_chain.invoke({\"question\": \"탄소의 원자 번호는 무엇인가요?\"})\n",
    "\n",
    "# 결과는 Pydantic 객체로 반환됨\n",
    "print(f\"분류: {result.category}\")\n",
    "print(f\"이유: {result.reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 언어 감지 체인`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "# Pydantic 모델로 구조화된 출력 정의\n",
    "class LanguageDetection(BaseModel):\n",
    "    \"\"\"텍스트의 언어 감지 결과\"\"\"\n",
    "\n",
    "    detected_language: Literal[\"영어(English)\", \"한국어(Korean)\", \"스페인어(Spanish)\", \"중국어(Chinese)\", \"일본어(Japanese)\", \"기타(Others)\"] = Field(\n",
    "         description=\"질문에 사용된 메인 언어\"\n",
    "    )    \n",
    "    explanation: str = Field(\n",
    "        description=\"언어 감지 근거에 대한 짧은 설명\"\n",
    "    )\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "language_detection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"당신은 언어 감지 전문가다. \n",
    "        주어진 텍스트의 언어를 정확하게 식별하고 분석해야 한다.\n",
    "        \n",
    "        다음 언어들을 주로 구분한다:\n",
    "        - English: 영어\n",
    "        - Korean: 한국어\n",
    "        - Spanish: 스페인어\n",
    "        - Chinese: 중국어\n",
    "        - Japanese: 일본어\n",
    "        - Others: 기타 언어\n",
    "        \n",
    "        텍스트의 문자 체계, 단어 패턴, 문법 구조를 분석하여 언어를 판별한다.\"\"\"\n",
    "    ),\n",
    "    (\"human\", \"다음 텍스트의 언어를 감지하세요: {question}\")\n",
    "])\n",
    "\n",
    "# 구조화된 출력을 사용한 체인 구성\n",
    "structured_llm = llm.with_structured_output(LanguageDetection)\n",
    "language_chain = language_detection_prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시\n",
    "examples = [\n",
    "    \"What is the atomic number of carbon?\",\n",
    "    \"탄소의 원자 번호는 무엇인가요?\",\n",
    "    \"碳的原子序数是多少？\",\n",
    "    \"炭素の原子番号は何ですか？\",\n",
    "]\n",
    "\n",
    "# 각 예시 처리\n",
    "for example in examples:\n",
    "    result = language_chain.invoke({\"question\": example})\n",
    "    \n",
    "    print(f\"\\n입력: {example}\")\n",
    "    print(f\"언어: {result.detected_language}\")\n",
    "    print(f\"설명: {result.explanation}\")\n",
    "    print(f\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) RunnableParallel을 사용한 병렬 실행 체인`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 관련된 분야를 찾아서 질문에 대한 답변을 생성하는 프롬프트\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "# 답변 템플릿 정의\n",
    "answer_template = \"\"\"\n",
    "당신은 {topic} 분야의 전문가입니다. {topic}에 관한 질문에 {language}로 답변해주세요.\n",
    "질문: {question}\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 및 체인 구성\n",
    "answer_prompt = PromptTemplate.from_template(answer_template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 병렬 처리 체인 구성\n",
    "answer_chain = RunnableParallel({\n",
    "    \"topic\": classification_chain | RunnableLambda(lambda x: x.category),            # 주제 분류 체인\n",
    "    \"language\": language_chain | RunnableLambda(lambda x: x.detected_language),         # 언어 감지 체인\n",
    "    \"question\": itemgetter(\"question\")  # 원본 질문 추출\n",
    "}) | answer_prompt | llm | output_parser\n",
    "\n",
    "# 체인 실행 예시\n",
    "result = answer_chain.invoke({\n",
    "    \"question\": \"탄소의 원자 번호는 무엇인가요?\"\n",
    "})\n",
    "\n",
    "print(\"처리 결과:\")\n",
    "print(f\"답변: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 RunnablePassthrough (입력을 출력으로 그대로 전달)\n",
    "\n",
    "- **RunnablePassthrough**는 입력값을 그대로 전달하여 원본 데이터를 보존\n",
    "\n",
    "- **RunnableParallel**과 함께 사용되어 입력 데이터를 새로운 키로 매핑 가능\n",
    "\n",
    "- **투명한 데이터 흐름**으로 파이프라인 디버깅과 구성이 용이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import re\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=lambda x: int(re.search(r'\\d+', x[\"query\"]).group()),\n",
    ")\n",
    "\n",
    "runnable.invoke({\"query\": '탄소의 원자 번호는 6입니다.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=lambda x: int(re.search(r'\\d+', x).group()),\n",
    ")\n",
    "\n",
    "runnable.invoke('탄소의 원자 번호는 6입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 실습 문제\n",
    "\n",
    "#### Gradio ChatInterface  \n",
    "- 설치: uv add gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 기본 구조`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# 챗봇 함수 정의\n",
    "def chat_function(message, history):\n",
    "    return message\n",
    "\n",
    "# 챗봇 인터페이스 생성\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_function,  # 실행할 함수\n",
    "    analytics_enabled=False,  # 사용 정보 제공 여부\n",
    "    type=\"messages\",\n",
    ")\n",
    "\n",
    "# 챗봇 인터페이스 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인터페이스 종료\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 문제: RunnableParallel 활용`\n",
    "- 하나의 입력에 대해 번역과 요약을 동시에 수행하는 체인을 만드세요.\n",
    "- gradio 인터페이스를 활용하여 챗봇 형태로 구현하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from gradio import ChatInterface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>💡 정답 보기</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from gradio import ChatInterface\n",
    "\n",
    "# 모델 정의\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.5)\n",
    "\n",
    "# 번역 체인\n",
    "translate_prompt = PromptTemplate.from_template(\n",
    "    \"다음 텍스트를 영어로 번역하세요: {text}\"\n",
    ")\n",
    "translate_chain = translate_prompt | model | StrOutputParser()\n",
    "\n",
    "# 요약 체인  \n",
    "summarize_prompt = PromptTemplate.from_template(\n",
    "    \"다음 텍스트를 한 문장으로 요약하세요: {text}\"\n",
    ")\n",
    "summarize_chain = summarize_prompt | model | StrOutputParser()\n",
    "\n",
    "# 병렬 처리 체인\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"translation\": translate_chain,\n",
    "    \"summary\": summarize_chain\n",
    "})\n",
    "\n",
    "# 테스트\n",
    "result = parallel_chain.invoke({\n",
    "    \"text\": \"안녕하세요. 오늘은 정말 좋은 날씨입니다. 산책하기 딱 좋네요.\"\n",
    "})\n",
    "print(f\"번역: {result['translation']}\")\n",
    "print(f\"요약: {result['summary']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chat_function(message, history):\n",
    "\n",
    "    result = parallel_chain.invoke({\"text\": message})\n",
    "    translation = result['translation']\n",
    "    summary = result['summary']\n",
    "\n",
    "    return f\"*번역*\\n{translation}\\n\\n*요약*\\n{summary}\"\n",
    "\n",
    "    \n",
    "demo = ChatInterface(\n",
    "    fn=chat_function,\n",
    "    title=\"번역 및 요약 챗봇\",\n",
    "    description=\"원하는 텍스트를 입력하면 번역과 요약을 동시에 제공합니다.\",\n",
    "    analytics_enabled=False,\n",
    "    type=\"messages\",\n",
    "    examples=[\n",
    "        \"안녕하세요. 오늘은 정말 좋은 날씨입니다. 산책하기 딱 좋네요.\",\n",
    "    ]\n",
    ")\n",
    "```\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
