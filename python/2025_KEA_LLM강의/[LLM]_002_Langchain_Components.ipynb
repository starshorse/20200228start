{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LangChainì˜ ê°œë…ê³¼ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì´í•´\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LangChain ì†Œê°œ\n",
    "\n",
    "- **LangChain**ì€ ëŒ€í™”í˜• AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ ê°œë°œí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "- **í•µì‹¬ ê°€ì¹˜**\n",
    "    - **ëª¨ë“ˆí™”**: ë…ë¦½ì ì¸ ì»´í¬ë„ŒíŠ¸ë¥¼ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ AI ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "    - **ìƒí˜¸ìš´ìš©ì„±**: ë‹¤ì–‘í•œ AI ëª¨ë¸ê³¼ ë°ì´í„° ì†ŒìŠ¤ë¥¼ í•˜ë‚˜ì˜ ì¸í„°í˜ì´ìŠ¤ë¡œ í†µí•©\n",
    "    - **í™•ì¥ì„±**: ê°„ë‹¨í•œ ì±—ë´‡ë¶€í„° ë³µì¡í•œ AI ì—ì´ì „íŠ¸ê¹Œì§€ í™•ì¥ ê°€ëŠ¥\n",
    "    - **ê´€ì°°ì„±**: LangSmithë¥¼ í†µí•œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…\n",
    "\n",
    "- **í•µì‹¬ ì•„í‚¤í…ì²˜**\n",
    "    ```markdown\n",
    "    â”œâ”€â”€ langchain-core     # ê¸°ë³¸ ì¶”ìƒí™” ë° ì¸í„°í˜ì´ìŠ¤\n",
    "    â”œâ”€â”€ langchain          # ì²´ì¸, ì—ì´ì „íŠ¸, ê²€ìƒ‰ ì „ëµ\n",
    "    â”œâ”€â”€ langchain-openai   # OpenAI í†µí•©\n",
    "    â”œâ”€â”€ LangGraph          # ë³µì¡í•œ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°\n",
    "    â””â”€â”€ LangSmith          # ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://python.langchain.com/svg/langchain_stack_112024_dark.svg\" \n",
    "        alt=\"langchain_stack\" \n",
    "        width=\"600\" \n",
    "        style=\"border: 0;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LangSmith ëª¨ë‹ˆí„°ë§**\n",
    "\n",
    "    - **LangSmith**ëŠ” LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ê´€ì°°ì„±(Observability)ì„ ì œê³µí•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "    - **ì£¼ìš” ê¸°ëŠ¥**\n",
    "        - **ì²´ì¸ ì‹¤í–‰ ë¡œê¹… ë° ì¶”ì **\n",
    "        - **í”„ë¡¬í”„íŠ¸ ë””ë²„ê¹…**\n",
    "        - **ì„±ëŠ¥ ì¸¡ì • ë° ë¶„ì„**\n",
    "        - **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**\n",
    "\n",
    "    - **ê³„ì • ê°€ì… ë° ì„¤ì •**\n",
    "        ```python\n",
    "        # 1. LangSmith ê³„ì • ê°€ì…: https://www.langchain.com/langsmith\n",
    "        # 2. .env íŒŒì¼ ì„¤ì •\n",
    "        LANGSMITH_API_KEY=your_langsmith_api_key\n",
    "        LANGSMITH_TRACING=true\n",
    "        LANGSMITH_PROJECT=your_project_name\n",
    "\n",
    "        # 3. í™˜ê²½ í™•ì¸\n",
    "        from dotenv import load_dotenv\n",
    "        import os\n",
    "\n",
    "        load_dotenv()\n",
    "        print(f\"LangSmith ì¶”ì  ìƒíƒœ: {os.getenv('LANGSMITH_TRACING')}\")\n",
    "        print(f\"í”„ë¡œì íŠ¸ëª…: {os.getenv('LANGSMITH_PROJECT')}\")\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "- **ì„¤ì¹˜**\n",
    "\n",
    "    ```bash\n",
    "    # pip ì„¤ì¹˜\n",
    "    pip install langchain langchain-openai langchain-google-genai\n",
    "\n",
    "    # uv ì„¤ì¹˜ \n",
    "    uv add langchain langchain-openai langchain-google-genai\n",
    "\n",
    "    # ì¶”ê°€ ë„êµ¬ pip ì„¤ì¹˜ (ì„ íƒì‚¬í•­)\n",
    "    pip install langchain-ollama langsmith\n",
    "\n",
    "    # ì¶”ê°€ ë„êµ¬ uv ì„¤ì¹˜ (ì„ íƒì‚¬í•­)\n",
    "    uv add langchain-ollama langsmith\n",
    "    ``` \n",
    "\n",
    "- **API í‚¤ ì„¤ì •**\n",
    "\n",
    "    ```python\n",
    "    # .env íŒŒì¼ ìƒì„±\n",
    "    OPENAI_API_KEY=your_openai_api_key_here\n",
    "    GOOGLE_API_KEY=your_google_api_key_here\n",
    "\n",
    "    # LangSmith ì„¤ì • (ì„ íƒì‚¬í•­)\n",
    "    LANGSMITH_API_KEY=your_langsmith_api_key\n",
    "    LANGSMITH_TRACING=true\n",
    "    LANGSMITH_PROJECT=your_project_name\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith ì¶”ì  í™•ì¸\n",
    "import os\n",
    "print(f\"LangSmith ì¶”ì : {os.getenv('LANGSMITH_TRACING')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. í•µì‹¬ ì»´í¬ë„ŒíŠ¸\n",
    "\n",
    "### 3.1 Chat Models (ì±„íŒ… ëª¨ë¸)\n",
    "\n",
    "- OpenAI, Anthropic, Google ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì§€ì›\n",
    "- í…ìŠ¤íŠ¸ ìƒì„±, ëŒ€í™”, ìš”ì•½ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\", \n",
    "    temperature=0.3,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "# ê°„ë‹¨í•œ ëŒ€í™”\n",
    "response = model.invoke(\"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ë‹µë³€: {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ë©”íƒ€ë°ì´í„°: {response.response_metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Messages (ë©”ì‹œì§€)\n",
    "\n",
    "- ë©”ì‹œì§€ëŠ” AIì™€ì˜ ëŒ€í™”ì—ì„œ ì—­í• ì„ êµ¬ë¶„í•˜ëŠ” ê¸°ë³¸ ë‹¨ìœ„ì…ë‹ˆë‹¤.\n",
    "- ë©”ì‹œì§€ëŠ” ì‚¬ìš©ì, AI, ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ ì—­í• ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# ì‹œìŠ¤í…œ ë©”ì‹œì§€: AIì˜ ì—­í•  ì •ì˜\n",
    "system_msg = SystemMessage(content=\"ë‹¹ì‹ ì€ ì¹œì ˆí•œ í™”í•™ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "# ì‚¬ìš©ì ë©”ì‹œì§€\n",
    "human_msg = HumanMessage(content=\"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ëª‡ ë²ˆì¸ê°€ìš”?\")\n",
    "\n",
    "# ëŒ€í™” ì‹¤í–‰\n",
    "messages = [system_msg, human_msg]\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Prompt Templates (í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿)\n",
    "\n",
    "- í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ ì¼ê´€ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ë³€ìˆ˜ ì¹˜í™˜ì„ í†µí•´ ë™ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) ê¸°ë³¸ í…œí”Œë¦¿`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# ì „ë¬¸ê°€ í…œí”Œë¦¿\n",
    "template = \"\"\"\n",
    "ë‹¹ì‹ ì€ {topic} ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. {topic}ì— ê´€í•œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "ì§ˆë¬¸: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# í…œí”Œë¦¿ ì…ë ¥ ë³€ìˆ˜ í™•ì¸\n",
    "print(f\"í•„ìˆ˜ ë³€ìˆ˜: {prompt.input_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…œí”Œë¦¿ í™•ì¸\n",
    "print(f\"í…œí”Œë¦¿: {prompt.template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…œí”Œë¦¿ ì‚¬ìš©\n",
    "formatted_prompt = prompt.format(\n",
    "    topic=\"í™”í•™\",\n",
    "    question=\"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    ")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±\n",
    "response = model.invoke(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) ì±„íŒ… í…œí”Œë¦¿`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ì±„íŒ…ìš© í…œí”Œë¦¿\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë‹¹ì‹ ì€ ì „ë¬¸ {subject} ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# í…œí”Œë¦¿ ì‚¬ìš©\n",
    "prompt = chat_template.format_messages(\n",
    "    subject=\"ì§„ë¡œ\",\n",
    "    question=\"ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ê°€ ë˜ë ¤ë©´ ì–´ë–¤ ê³µë¶€ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…œí”Œë¦¿ í™•ì¸\n",
    "pprint(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±\n",
    "response = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. LCEL (LangChain Expression Language)\n",
    "\n",
    "### 4.1 LCELì´ë€?\n",
    "\n",
    "- **LCEL**ì€ `|` ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” ì„ ì–¸ì  ì²´ì´ë‹ì„ ì§€ì›\n",
    "\n",
    "- **í•µì‹¬ íŠ¹ì§•**\n",
    "    - **ì¬ì‚¬ìš©ì„±**: ì •ì˜ëœ ì²´ì¸ì„ ë‹¤ë¥¸ ì²´ì¸ì˜ ì»´í¬ë„ŒíŠ¸ë¡œ í™œìš©\n",
    "    - **ë‹¤ì–‘í•œ ì‹¤í–‰ ë°©ì‹**: `.invoke()`, `.batch()`, `.stream()`, `.astream()`\n",
    "    - **ìë™ ìµœì í™”**: ë°°ì¹˜ ì²˜ë¦¬ ì‹œ íš¨ìœ¨ì ì¸ ì‘ì—… ìˆ˜í–‰\n",
    "    - **ìŠ¤í‚¤ë§ˆ ì§€ì›**: ì…ë ¥/ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ìë™ ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ê¸°ë³¸ ì²´ì¸ êµ¬ì„±\n",
    "\n",
    "`(1) Prompt + LLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ì»´í¬ë„ŒíŠ¸ ì •ì˜\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"ë‹¹ì‹ ì€ {topic} ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. {topic}ì— ê´€í•œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\\n\"\n",
    "    \"ì§ˆë¬¸: {question}\"\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.3)\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±\n",
    "chain = prompt | llm\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "response = chain.invoke({\n",
    "    \"topic\": \"í™”í•™\",\n",
    "    \"question\": \"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "})\n",
    "\n",
    "print(f\"ë‹µë³€: {response.content}\")  # AIMessage ê°ì²´ë¡œ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) Prompt + LLM + Output Parser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ì¶œë ¥ íŒŒì„œ ì¶”ê°€\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# ì™„ì „í•œ ì²´ì¸ êµ¬ì„±\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰ (ë¬¸ìì—´ ë°˜í™˜)\n",
    "response = chain.invoke({\n",
    "    \"topic\": \"í™”í•™\",\n",
    "    \"question\": \"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "})\n",
    "\n",
    "print(f\"ë‹µë³€: {response}\")  # ë¬¸ìì—´ë¡œ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Runnable\n",
    "\n",
    "* ì‹¤í–‰ ì¸í„°í˜ì´ìŠ¤: ëª¨ë“  LangChain ì»´í¬ë„ŒíŠ¸ëŠ” Runnable ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ì—¬ ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰\n",
    "\n",
    "* ì‹¤í–‰ ë©”ì„œë“œ: `.invoke()`, `.batch()`, `.stream()`, `.astream()` ë“± ë‹¤ì–‘í•œ ì‹¤í–‰ ë°©ì‹ì„ ì œê³µ\n",
    "\n",
    "* í˜¸í™˜ì„±: ëª¨ë“  Runnable ì»´í¬ë„ŒíŠ¸ëŠ” íŒŒì´í”„(|) ì—°ì‚°ìë¥¼ í†µí•´ ì—°ê²° ê°€ëŠ¥í•˜ë©°, ì¬ì‚¬ìš©ì´ ìš©ì´\n",
    "\n",
    "* Runnableì˜ ì£¼ìš” ìœ í˜•:\n",
    "\n",
    "    * `RunnableSequence`: ì—¬ëŸ¬ Runnableì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰\n",
    "    * `RunnablePassthrough`: ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬    \n",
    "    * `RunnableParallel`: ì—¬ëŸ¬ Runnableì„ ë³‘ë ¬ë¡œ ì‹¤í–‰\n",
    "    * `RunnableLambda`: íŒŒì´ì¬ í•¨ìˆ˜ë¥¼ Runnableë¡œ ë˜í•‘í•˜ì—¬ ì²´ì¸ì—ì„œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 RunnableSequence (ìˆœì°¨ ì‹¤í–‰)\n",
    "\n",
    "- **RunnableSequence**ëŠ” ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì—°ê²°í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì²´ì¸\n",
    "\n",
    "- `|` ì—°ì‚°ìë¡œ ì—°ê²°ëœ ê° ë‹¨ê³„ì˜ **ì¶œë ¥ì´ ë‹¤ìŒ ë‹¨ê³„ì˜ ì…ë ¥**ìœ¼ë¡œ ì „ë‹¬\n",
    "\n",
    "- **ë‹¤ì–‘í•œ ì‹¤í–‰ ë°©ì‹**(ë™ê¸°/ë¹„ë™ê¸°, ë°°ì¹˜/ìŠ¤íŠ¸ë¦¬ë°)ì„ ì§€ì›\n",
    "\n",
    "- LLM ì²´ì¸, ë°ì´í„° íŒŒì´í”„ë¼ì¸, ìë™í™”ëœ ì‘ì—… ë“± **ë‹¤ë‹¨ê³„ ì²˜ë¦¬**ì— í™œìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²ˆì—­ ì „ìš© ì²´ì¸ (íŒŒì´í”„ ì—°ì‚°ì ì‚¬ìš©)\n",
    "translation_prompt = PromptTemplate.from_template(\n",
    "    \"'{text}'ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë²ˆì—­ëœ ë¬¸ì¥ë§Œì„ ì¶œë ¥í•´ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "translation_chain = translation_prompt | llm | StrOutputParser()\n",
    "\n",
    "# ë²ˆì—­ ì‹¤í–‰\n",
    "result = translation_chain.invoke({\"text\": \"ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# ëª…ì‹œì  RunnableSequence ìƒì„±\n",
    "translation_chain = RunnableSequence(\n",
    "    first=translation_prompt, \n",
    "    middle=[llm],\n",
    "    last=StrOutputParser()\n",
    ")\n",
    "\n",
    "# íŒŒì´í”„ ì—°ì‚°ìì™€ ë™ì¼í•œ ê²°ê³¼\n",
    "# translation_chain = translation_prompt | llm | StrOutputParser()\n",
    "\n",
    "result = translation_chain.invoke({\"text\": \"ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 RunnableLambda (í•¨ìˆ˜ ë˜í•‘)\n",
    "\n",
    "- **RunnableLambda**ëŠ” ì¼ë°˜ í•¨ìˆ˜ë¥¼ Runnable ê°ì²´ë¡œ ë³€í™˜í•˜ëŠ” ë˜í¼ ì»´í¬ë„ŒíŠ¸\n",
    "\n",
    "- ì²´ì¸ì— **ì»¤ìŠ¤í…€ ë¡œì§**ì„ ì‰½ê²Œ í†µí•©í•  ìˆ˜ ìˆì–´ ë°ì´í„° ì „ì²˜ë¦¬, í›„ì²˜ë¦¬ì— ìœ ìš©\n",
    "\n",
    "- `|` ì—°ì‚°ìë¡œ ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ë“¤ê³¼ ì—°ê²°í•´ **ë³µì¡í•œ ì²˜ë¦¬ íë¦„**ì„ êµ¬ì„± ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜\n",
    "def extract_number(query):\n",
    "    return int(re.search(r'\\d+', query).group())\n",
    "\n",
    "# RunnablePassthroughë¡œ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ê³ , RunnableLambdaë¡œ ìˆ«ì ì¶”ì¶œ í•¨ìˆ˜ ì‹¤í–‰\n",
    "runnable = RunnableLambda(extract_number)\n",
    "\n",
    "# ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ 6ì„ ì¶”ì¶œ\n",
    "result = runnable.invoke('íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” 6ì…ë‹ˆë‹¤.')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\" ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  ì–‘ìª½ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤. \"\"\"\n",
    "    return text.strip().lower()\n",
    "\n",
    "# í›„ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def postprocess_response(response: AIMessage) -> dict:\n",
    "    \"\"\" ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ëŒ€ë¬¸ìë¡œ ë³€í™˜í•˜ê³  ê¸¸ì´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. \"\"\"\n",
    "    response_text = response.content\n",
    "    return {\n",
    "        \"processed_response\": response_text.upper(),\n",
    "        \"length\": len(response_text)\n",
    "    }\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "prompt = ChatPromptTemplate.from_template(\"ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ ì˜ì–´ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”: {topic}\")\n",
    "\n",
    "# ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±\n",
    "chain = (\n",
    "    RunnableLambda(preprocess_text) |  # ì…ë ¥ ì „ì²˜ë¦¬\n",
    "    prompt |                           # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…\n",
    "    llm |                              # LLM ì¶”ë¡ \n",
    "    RunnableLambda(postprocess_response)  # ì¶œë ¥ í›„ì²˜ë¦¬\n",
    ")\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "result = chain.invoke(\"  Artificial Intelligence  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ì‘ë‹µ ê¸¸ì´: {result['length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ì²˜ë¦¬ëœ ì‘ë‹µ: {result['processed_response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 RunnableParallel (ë³‘ë ¬ ì‹¤í–‰)\n",
    "\n",
    "- **RunnableParallel**ì€ ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ êµ¬ì„±í•´ **ë™ì‹œ ì‹¤í–‰**\n",
    "\n",
    "- ë™ì¼í•œ ì…ë ¥ì´ ëª¨ë“  ë³‘ë ¬ ì»´í¬ë„ŒíŠ¸ì— ì „ë‹¬ë˜ë©°, ê²°ê³¼ëŠ” **í‚¤-ê°’ ìŒ**ìœ¼ë¡œ ë°˜í™˜\n",
    "\n",
    "- **ë°ì´í„° ë³€í™˜**ê³¼ **íŒŒì´í”„ë¼ì¸ êµ¬ì„±**ì— íŠ¹í™”ë˜ì–´ ìˆìœ¼ë©°, ì¶œë ¥ í˜•ì‹ì„ ë‹¤ìŒ ë‹¨ê³„ì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) ì§ˆë¬¸ ë¶„ì„ ì²´ì¸`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "# Pydantic ëª¨ë¸ë¡œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ì •ì˜\n",
    "class SubjectClassification(BaseModel):\n",
    "    \"\"\"ì§ˆë¬¸ì˜ ì£¼ì œ ë¶„ë¥˜ ê²°ê³¼\"\"\"\n",
    "    category: Literal[\"í™”í•™(Chemistry)\", \"ë¬¼ë¦¬(Physics)\", \"ìƒë¬¼(Biology)\"] = Field(\n",
    "        description=\"ì§ˆë¬¸ì´ ì†í•˜ëŠ” ê³¼í•™ ë¶„ì•¼ ì¹´í…Œê³ ë¦¬\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"ë¶„ë¥˜ ì´ìœ ì— ëŒ€í•œ ì§§ì€ ì„¤ëª…\"\n",
    "    )\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "classification_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"ë‹¹ì‹ ì€ ì§ˆë¬¸ì„ ê³¼í•™ ë¶„ì•¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ë‹¤.\n",
    "        ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  í•´ë‹¹í•˜ëŠ” ì¹´í…Œê³ ë¦¬ë¡œ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•´ì•¼ í•œë‹¤.\n",
    "        \n",
    "        ë¶„ë¥˜ ê¸°ì¤€:\n",
    "        - í™”í•™(Chemistry): ì›ì†Œ, í™”í•©ë¬¼, ë°˜ì‘, ë¶„ì êµ¬ì¡° ë“±\n",
    "        - ë¬¼ë¦¬(Physics): í˜, ìš´ë™, ì—ë„ˆì§€, íŒŒë™, ì „ê¸° ë“±  \n",
    "        - ìƒë¬¼(Biology): ìƒëª…ì²´, ì„¸í¬, ìœ ì „, ìƒíƒœê³„ ë“±\"\"\"\n",
    "    ),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì‚¬ìš©í•œ ì²´ì¸ êµ¬ì„±\n",
    "structured_llm = llm.with_structured_output(SubjectClassification)\n",
    "classification_chain = classification_prompt | structured_llm\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "result = classification_chain.invoke({\"question\": \"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"})\n",
    "\n",
    "# ê²°ê³¼ëŠ” Pydantic ê°ì²´ë¡œ ë°˜í™˜ë¨\n",
    "print(f\"ë¶„ë¥˜: {result.category}\")\n",
    "print(f\"ì´ìœ : {result.reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) ì–¸ì–´ ê°ì§€ ì²´ì¸`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "# Pydantic ëª¨ë¸ë¡œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ì •ì˜\n",
    "class LanguageDetection(BaseModel):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ ê°ì§€ ê²°ê³¼\"\"\"\n",
    "\n",
    "    detected_language: Literal[\"ì˜ì–´(English)\", \"í•œêµ­ì–´(Korean)\", \"ìŠ¤í˜ì¸ì–´(Spanish)\", \"ì¤‘êµ­ì–´(Chinese)\", \"ì¼ë³¸ì–´(Japanese)\", \"ê¸°íƒ€(Others)\"] = Field(\n",
    "         description=\"ì§ˆë¬¸ì— ì‚¬ìš©ëœ ë©”ì¸ ì–¸ì–´\"\n",
    "    )    \n",
    "    explanation: str = Field(\n",
    "        description=\"ì–¸ì–´ ê°ì§€ ê·¼ê±°ì— ëŒ€í•œ ì§§ì€ ì„¤ëª…\"\n",
    "    )\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "language_detection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"ë‹¹ì‹ ì€ ì–¸ì–´ ê°ì§€ ì „ë¬¸ê°€ë‹¤. \n",
    "        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ì •í™•í•˜ê²Œ ì‹ë³„í•˜ê³  ë¶„ì„í•´ì•¼ í•œë‹¤.\n",
    "        \n",
    "        ë‹¤ìŒ ì–¸ì–´ë“¤ì„ ì£¼ë¡œ êµ¬ë¶„í•œë‹¤:\n",
    "        - English: ì˜ì–´\n",
    "        - Korean: í•œêµ­ì–´\n",
    "        - Spanish: ìŠ¤í˜ì¸ì–´\n",
    "        - Chinese: ì¤‘êµ­ì–´\n",
    "        - Japanese: ì¼ë³¸ì–´\n",
    "        - Others: ê¸°íƒ€ ì–¸ì–´\n",
    "        \n",
    "        í…ìŠ¤íŠ¸ì˜ ë¬¸ì ì²´ê³„, ë‹¨ì–´ íŒ¨í„´, ë¬¸ë²• êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì–¸ì–´ë¥¼ íŒë³„í•œë‹¤.\"\"\"\n",
    "    ),\n",
    "    (\"human\", \"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•˜ì„¸ìš”: {question}\")\n",
    "])\n",
    "\n",
    "# êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì‚¬ìš©í•œ ì²´ì¸ êµ¬ì„±\n",
    "structured_llm = llm.with_structured_output(LanguageDetection)\n",
    "language_chain = language_detection_prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "examples = [\n",
    "    \"What is the atomic number of carbon?\",\n",
    "    \"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"ç¢³çš„åŸå­åºæ•°æ˜¯å¤šå°‘ï¼Ÿ\",\n",
    "    \"ç‚­ç´ ã®åŸå­ç•ªå·ã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
    "]\n",
    "\n",
    "# ê° ì˜ˆì‹œ ì²˜ë¦¬\n",
    "for example in examples:\n",
    "    result = language_chain.invoke({\"question\": example})\n",
    "    \n",
    "    print(f\"\\nì…ë ¥: {example}\")\n",
    "    print(f\"ì–¸ì–´: {result.detected_language}\")\n",
    "    print(f\"ì„¤ëª…: {result.explanation}\")\n",
    "    print(f\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) RunnableParallelì„ ì‚¬ìš©í•œ ë³‘ë ¬ ì‹¤í–‰ ì²´ì¸`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¶„ì•¼ë¥¼ ì°¾ì•„ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "# ë‹µë³€ í…œí”Œë¦¿ ì •ì˜\n",
    "answer_template = \"\"\"\n",
    "ë‹¹ì‹ ì€ {topic} ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. {topic}ì— ê´€í•œ ì§ˆë¬¸ì— {language}ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "ì§ˆë¬¸: {question}\n",
    "\"\"\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸ êµ¬ì„±\n",
    "answer_prompt = PromptTemplate.from_template(answer_template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# ë³‘ë ¬ ì²˜ë¦¬ ì²´ì¸ êµ¬ì„±\n",
    "answer_chain = RunnableParallel({\n",
    "    \"topic\": classification_chain | RunnableLambda(lambda x: x.category),            # ì£¼ì œ ë¶„ë¥˜ ì²´ì¸\n",
    "    \"language\": language_chain | RunnableLambda(lambda x: x.detected_language),         # ì–¸ì–´ ê°ì§€ ì²´ì¸\n",
    "    \"question\": itemgetter(\"question\")  # ì›ë³¸ ì§ˆë¬¸ ì¶”ì¶œ\n",
    "}) | answer_prompt | llm | output_parser\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰ ì˜ˆì‹œ\n",
    "result = answer_chain.invoke({\n",
    "    \"question\": \"íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "})\n",
    "\n",
    "print(\"ì²˜ë¦¬ ê²°ê³¼:\")\n",
    "print(f\"ë‹µë³€: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 RunnablePassthrough (ì…ë ¥ì„ ì¶œë ¥ìœ¼ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬)\n",
    "\n",
    "- **RunnablePassthrough**ëŠ” ì…ë ¥ê°’ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ì—¬ ì›ë³¸ ë°ì´í„°ë¥¼ ë³´ì¡´\n",
    "\n",
    "- **RunnableParallel**ê³¼ í•¨ê»˜ ì‚¬ìš©ë˜ì–´ ì…ë ¥ ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ í‚¤ë¡œ ë§¤í•‘ ê°€ëŠ¥\n",
    "\n",
    "- **íˆ¬ëª…í•œ ë°ì´í„° íë¦„**ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹…ê³¼ êµ¬ì„±ì´ ìš©ì´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import re\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=lambda x: int(re.search(r'\\d+', x[\"query\"]).group()),\n",
    ")\n",
    "\n",
    "runnable.invoke({\"query\": 'íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” 6ì…ë‹ˆë‹¤.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=lambda x: int(re.search(r'\\d+', x).group()),\n",
    ")\n",
    "\n",
    "runnable.invoke('íƒ„ì†Œì˜ ì›ì ë²ˆí˜¸ëŠ” 6ì…ë‹ˆë‹¤.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. ì‹¤ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### Gradio ChatInterface  \n",
    "- ì„¤ì¹˜: uv add gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) ê¸°ë³¸ êµ¬ì¡°`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# ì±—ë´‡ í•¨ìˆ˜ ì •ì˜\n",
    "def chat_function(message, history):\n",
    "    return message\n",
    "\n",
    "# ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_function,  # ì‹¤í–‰í•  í•¨ìˆ˜\n",
    "    analytics_enabled=False,  # ì‚¬ìš© ì •ë³´ ì œê³µ ì—¬ë¶€\n",
    "    type=\"messages\",\n",
    ")\n",
    "\n",
    "# ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸í„°í˜ì´ìŠ¤ ì¢…ë£Œ\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) ë¬¸ì œ: RunnableParallel í™œìš©`\n",
    "- í•˜ë‚˜ì˜ ì…ë ¥ì— ëŒ€í•´ ë²ˆì—­ê³¼ ìš”ì•½ì„ ë™ì‹œì— ìˆ˜í–‰í•˜ëŠ” ì²´ì¸ì„ ë§Œë“œì„¸ìš”.\n",
    "- gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ì±—ë´‡ í˜•íƒœë¡œ êµ¬í˜„í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from gradio import ChatInterface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from gradio import ChatInterface\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.5)\n",
    "\n",
    "# ë²ˆì—­ ì²´ì¸\n",
    "translate_prompt = PromptTemplate.from_template(\n",
    "    \"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”: {text}\"\n",
    ")\n",
    "translate_chain = translate_prompt | model | StrOutputParser()\n",
    "\n",
    "# ìš”ì•½ ì²´ì¸  \n",
    "summarize_prompt = PromptTemplate.from_template(\n",
    "    \"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”: {text}\"\n",
    ")\n",
    "summarize_chain = summarize_prompt | model | StrOutputParser()\n",
    "\n",
    "# ë³‘ë ¬ ì²˜ë¦¬ ì²´ì¸\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"translation\": translate_chain,\n",
    "    \"summary\": summarize_chain\n",
    "})\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "result = parallel_chain.invoke({\n",
    "    \"text\": \"ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ì€ ì •ë§ ì¢‹ì€ ë‚ ì”¨ì…ë‹ˆë‹¤. ì‚°ì±…í•˜ê¸° ë”± ì¢‹ë„¤ìš”.\"\n",
    "})\n",
    "print(f\"ë²ˆì—­: {result['translation']}\")\n",
    "print(f\"ìš”ì•½: {result['summary']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chat_function(message, history):\n",
    "\n",
    "    result = parallel_chain.invoke({\"text\": message})\n",
    "    translation = result['translation']\n",
    "    summary = result['summary']\n",
    "\n",
    "    return f\"*ë²ˆì—­*\\n{translation}\\n\\n*ìš”ì•½*\\n{summary}\"\n",
    "\n",
    "    \n",
    "demo = ChatInterface(\n",
    "    fn=chat_function,\n",
    "    title=\"ë²ˆì—­ ë° ìš”ì•½ ì±—ë´‡\",\n",
    "    description=\"ì›í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ ë²ˆì—­ê³¼ ìš”ì•½ì„ ë™ì‹œì— ì œê³µí•©ë‹ˆë‹¤.\",\n",
    "    analytics_enabled=False,\n",
    "    type=\"messages\",\n",
    "    examples=[\n",
    "        \"ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ì€ ì •ë§ ì¢‹ì€ ë‚ ì”¨ì…ë‹ˆë‹¤. ì‚°ì±…í•˜ê¸° ë”± ì¢‹ë„¤ìš”.\",\n",
    "    ]\n",
    ")\n",
    "```\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
